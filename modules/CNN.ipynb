{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import numpy as np\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "from getdata import CrossValidation, GetData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom Image Dataset to import training data into PyTorch\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataset, labels, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        data = self.dataset[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing requires the absence of the labels, so it has it's own Class\n",
    "\n",
    "class TestImageDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.dataset[index]\n",
    "\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The image collector gets the data, and formats it correctly for usage. \n",
    "## We also perform any necessary additional transformations here\n",
    "\n",
    "class ImageGetter:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_modifier='_int',\n",
    "        as_image=False,\n",
    "        transform=False,\n",
    "        augment=False\n",
    "    ):\n",
    "        self.dataset_modifier = dataset_modifier\n",
    "        self.as_image = as_image\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "\n",
    "    def kaggle(self, mean, std_dev):\n",
    "        data_getter = GetData()\n",
    "\n",
    "        train_x, train_y = data_getter.load_training(\n",
    "            dataset_modifier=self.dataset_modifier,\n",
    "            as_image=self.as_image,\n",
    "            transform=self.transform,\n",
    "            augment=self.augment\n",
    "        )\n",
    "\n",
    "        test_x = data_getter.load_test(\n",
    "            as_image=self.as_image,\n",
    "            transform=self.transform\n",
    "        )\n",
    "\n",
    "        train_x = train_x.reshape((-1, 64, 64, 1))\n",
    "        train_y = train_y.flatten()\n",
    "        test_x = test_x.reshape((-1, 64, 64, 1))\n",
    "\n",
    "        print('dataset loaded successful for kaggle')\n",
    "\n",
    "        tensor_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((mean,), (std_dev,)),\n",
    "        ])\n",
    "\n",
    "        trainset = ImageDataset(\n",
    "            train_x,\n",
    "            train_y,\n",
    "            transform=tensor_transform\n",
    "        )\n",
    "\n",
    "        testset = TestImageDataset(\n",
    "            test_x,\n",
    "            transform=tensor_transform\n",
    "        )\n",
    "\n",
    "        trainloader = torch.utils.data.DataLoader(\n",
    "            trainset,\n",
    "            shuffle=True,\n",
    "            batch_size=8,\n",
    "            num_workers=2\n",
    "        )\n",
    "\n",
    "        testloader = torch.utils.data.DataLoader(\n",
    "            testset,\n",
    "            batch_size=8,\n",
    "            num_workers=2\n",
    "        )\n",
    "\n",
    "        return trainloader, testloader\n",
    "\n",
    "    def process(self, mean, std_dev):\n",
    "        cv = CrossValidation(\n",
    "            dataset_modifier=self.dataset_modifier,\n",
    "            transform=self.transform,\n",
    "            as_image=True,\n",
    "            augment=self.augment\n",
    "        )\n",
    "        train_x, train_y, valid_x, valid_y = cv.get_set()\n",
    "        print('data loaded successfully')\n",
    "\n",
    "        train_x = train_x.reshape((-1, 64, 64, 1))\n",
    "        valid_x = valid_x.reshape((-1, 64, 64, 1))\n",
    "\n",
    "        tensor_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((mean,), (std_dev,)),\n",
    "        ])\n",
    "\n",
    "        trainset = ImageDataset(\n",
    "            train_x,\n",
    "            train_y,\n",
    "            transform=tensor_transform\n",
    "        )\n",
    "\n",
    "        testset = ImageDataset(\n",
    "            valid_x,\n",
    "            valid_y,\n",
    "            transform=tensor_transform\n",
    "        )\n",
    "\n",
    "        trainloader = torch.utils.data.DataLoader(\n",
    "            trainset,\n",
    "            batch_size=8,\n",
    "            num_workers=4\n",
    "        )\n",
    "\n",
    "        testloader = torch.utils.data.DataLoader(\n",
    "            testset,\n",
    "            batch_size=8,\n",
    "            num_workers=4\n",
    "        )\n",
    "\n",
    "        return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The neural network itself, pretty simple\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(10, 10, 5)\n",
    "        self.fc1 = nn.Linear(10*13*13, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        print('NN initialized...')\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "\n",
    "        conv = self.conv1(x)\n",
    "        pool = self.pool(conv)\n",
    "        x = F.relu(pool)\n",
    "        #print(x.shape)\n",
    "\n",
    "        conv = self.conv2(x)\n",
    "        pool = self.pool(conv)\n",
    "        x = F.relu(pool)\n",
    "\n",
    "        #print(x.shape)\n",
    "        # 4 is the batch size\n",
    "        x = x.view(-1, 10*13*13)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "28 (8, 24)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAasAAAGoCAYAAAD4hcrDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAE/FJREFUeJzt3V+spHd5H/DvE/8NToIx0JWLae0KC0QrbNCKGIGiBNepS1HsC8siSqNVZGlvaAVVqtTkJmqVSHAT4ouIdgUkW4nEWJtQWwhBXMdRWim42LHLHxsEcY2wa3uhsRuaC4PJ04szhrW9x+fsnjnnPDvz+Uirmfedd+Z9fvbY3/3NPPN7q7sDAJP92H4XAABbEVYAjCesABhPWAEwnrACYDxhBcB4wgqA8YQVAOMJKwDGO3snT66qa5PckuSsJB/t7g++1PHn1nl9fi7YySkBWCHfzVPf6e5Xb3XcaYdVVZ2V5HeTXJPk0SRfqKo7uvvBzZ5zfi7IT9fVp3tKAFbMf+1j39zOcTv5GPCtSb7R3Q939/eS3Jrkuh28HgCc1E7C6jVJvnXC9qOLfc9TVYer6t6quvf7eWYHpwNgXe16g0V3H+nug9198Jyct9unA2AF7SSsHkvy2hO2L1nsA4Cl2klYfSHJ5VV1WVWdm+Q9Se5YTlkA8COn3Q3Y3c9W1b9K8rlstK5/vLu/srTKAGBhR7+z6u7PJPnMkmoBgJOyggUA4wkrAMYTVgCMJ6wAGE9YATCesAJgPGEFwHjCCoDxhBUA4wkrAMYTVgCMJ6wAGE9YATCesAJgPGEFwHjCCoDxhBUA4wkrAMYTVgCMJ6wAGE9YATDe2ftdwBnnqjct53U+/8XlvA7AGjCzAmA8YQXAeMIKgPGEFQDjCSsAxhNWAIyndX2/nGoLvFZ3YI2ZWQEwnrACYDxhBcB4wgqA8YQVAOOtRzfgshaf3U+bjUGXILAGzKwAGE9YATCesAJgPGEFwHjCCoDx1qMbcJXpEgTWgJkVAOMJKwDGE1YAjCesABhPWAEwnrACYDxhBcB4wgqA8YQVAOMJKwDGE1YAjLfl2oBV9fEk705yvLv/yWLfRUk+meTSJI8kubG7n9q9MtmUNQCBNbCdmdXvJ7n2BftuTnJXd1+e5K7FNgDsii3Dqrv/PMlfv2D3dUmOLu4fTXL9kusCgB863UuEHOjuxxf3n0hyYLMDq+pwksNJcn5edpqnA2Cd7bjBors7Sb/E40e6+2B3Hzwn5+30dACsodMNqyer6uIkWdweX15JAPB8pxtWdyQ5tLh/KMntyykHAF5sy7Cqqj9M8hdJXl9Vj1bVTUk+mOSaqvp6kn+62AaAXbFlg0V3/+ImD1295FoA4KSsYAHAeMIKgPGEFQDjne6PgpniqjedfL81A4EVYmYFwHjCCoDxhBUA4wkrAMYTVgCMtx7dgC/VGbdZNx0AY5hZATCesAJgPGEFwHjCCoDxhBUA461HN+BLmbaGnu5EWKrvHH7bSfe/6shf7OtrcWrMrAAYT1gBMJ6wAmA8YQXAeMIKgPF0AwIjbdZ5d6a8PstlZgXAeMIKgPGEFQDjCSsAxhNWAIynG3C/WAOQNbPK3XfWDNx9ZlYAjCesABhPWAEwnrACYDxhBcB4wgqA8bSuA0u1yi3qp0pL+/KYWQEwnrACYDxhBcB4wgqA8YQVAOPpBtwvn//iyfdb4BZWni7BU2dmBcB4wgqA8YQVAOMJKwDGE1YAjKcbcJpT7RLc7HjgjKNLcHNmVgCMJ6wAGE9YATCesAJgPGEFwHhbdgNW1WuT/OckB5J0kiPdfUtVXZTkk0kuTfJIkhu7+6ndK3XN6fpjGFcEZi9tZ2b1bJJf7e43JrkqyXur6o1Jbk5yV3dfnuSuxTYALN2WYdXdj3f3Xy7ufzfJQ0lek+S6JEcXhx1Ncv1uFQnAejulHwVX1aVJ3pzkniQHuvvxxUNPZONjwpM953CSw0lyfl52unUCsMa23WBRVT+R5I+SvL+7/+bEx7q7s/F91ot095HuPtjdB8/JeTsqFoD1tK2wqqpzshFUn+juP17sfrKqLl48fnGS47tTIgDrbsuwqqpK8rEkD3X3b5/w0B1JDi3uH0py+/LLA4DtfWf19iS/nORLVfXAYt+vJ/lgktuq6qYk30xy4+6UCMC62zKsuvu/J6lNHr56ueUAwItZwQKA8YQVAOMJKwDGc6Vg4LRsdvVaawaePlcE3pyZFQDjCSsAxhNWAIwnrAAYT1gBMJ5uQGCpdAluTdffqTOzAmA8YQXAeMIKgPGEFQDjCSsAxtMNuKK+8eGrlvZar/s3n1/aa7G+VrlLUHff7jOzAmA8M6vTdGcf2+8SXtr7h9fHnrqmbtjvEmBHzKwAGM/Maoem/o3Vd1YkZ8AnALBNZlYAjGdmdYZb5gwKTsUqdPFx5jCzAmA8YQXAeMIKgPGEFQDjCSsAxtMNeIbYz66/zc7t91dnHh18P2I9vzOLmRUA4wkrAMYTVgCMJ6wAGE9YATCesAJgPK3rw1iYFuDFzKwAGE9YATCesAJgPGEFwHjCCoDxdAPuslXo7rNg7ZnnhwvW/qdjz99eARagXU9mVgCMJ6wAGE9YATCesAJgPGEFwHi6AdmSy9oD+83MCoDxhBUA4wkrAMYTVgCMJ6wAGG/LbsCqOj/Jnyc5b3H8se7+jaq6LMmtSV6Z5L4kv9zd39vNYidbhTUAN6Prj2Wwph87sZ2Z1TNJ3tndVyS5Msm1VXVVkg8l+XB3vy7JU0lu2r0yAVhnW4ZVb/h/i81zFn86yTuTHFvsP5rk+l2pEIC1t63vrKrqrKp6IMnxJHcm+askT3f3s4tDHk3ymk2ee7iq7q2qe7+fZ5ZRMwBrZlth1d0/6O4rk1yS5K1J3rDdE3T3ke4+2N0Hz8l5p1kmAOvslLoBu/vpJHcneVuSC6vquQaNS5I8tuTaACDJ9roBX53k+939dFX9eJJrstFccXeSG7LREXgoye27WSiwfS/svHtue5lXDNbdx17azkK2Fyc5WlVnZWMmdlt3f7qqHkxya1X9ZpL7k3xsF+sEYI1tGVbd/cUkbz7J/oez8f0VAOwqK1gAMJ6wAmA8YQXAeK4UfIbYbH2+VV6TkOV7qQ6+zToFdf0xgZkVAOMJKwDGE1YAjCesABhPWAEwnm5AIImuP2YzswJgPGEFwHjCCoDxhBUA4wkrAMYTVgCMJ6wAGE9YATCesAJgPGEFwHjCCoDxrA14in54Zd73H3v+9pJsdkXgvbCf5wZ4KWZWAIwnrAAYT1gBMJ6wAmA8YQXAeLoBh1l2d+Eyzq1LEJbr/37mdSfd//J3fWOPKzlzmFkBMJ6wAmA8YQXAeMIKgPGEFQDj6QZkS6faoah7EF6arr9TZ2YFwHjCCoDxhBUA4wkrAMYTVgCMJ6wAGE/r+il6YVv2c9v7uQDtNFrdgWUzswJgPGEFwHjCCoDxhBUA4wkrAMbTDbgkm3W06RLc2mb/jHQJAs8xswJgPGEFwHjCCoDxhBUA4207rKrqrKq6v6o+vdi+rKruqapvVNUnq+rc3SsTgHV2Kt2A70vyUJKfWmx/KMmHu/vWqvqPSW5K8pEl13fGW4Uuwf0agy5BznTv/spTJ93/6X/8iqW8/uf+9wMn3f/P/v6VS3n9SbY1s6qqS5L8iyQfXWxXkncmObY45GiS63ejQADY7seAv5Pk15L83WL7lUme7u5nF9uPJnnNkmsDgCTbCKuqeneS49193+mcoKoOV9W9VXXv9/PM6bwEAGtuO99ZvT3JL1TVu5Kcn43vrG5JcmFVnb2YXV2S5LGTPbm7jyQ5kiQ/VRf1UqoGYK1sObPq7g909yXdfWmS9yT50+7+pSR3J7lhcdihJLfvWpUArLWdrA3475LcWlW/meT+JB9bTknrYVkdbcvsyDvVmlah0xF207K6/jazil1/mzmlsOruP0vyZ4v7Dyd56/JLAoDns4IFAOMJKwDGE1YAjCesABjPlYLPEGdSh50uQWDZzKwAGE9YATCesAJgPGEFwHjCCoDxdAOydLr+gGUzswJgPGEFwHjCCoDxhBUA4wkrAMYTVgCMJ6wAGE9YATCesAJgPGEFwHjCCoDxrA3IadvtNQA3u+IwsH7MrAAYT1gBMJ6wAmA8YQXAeMIKgPF0A54hNuuMc1VeYB2YWQEwnrACYDxhBcB4wgqA8YQVAOPpBjxD7EXXn85CYCozKwDGE1YAjCesABhPWAEwnrACYDxhBcB4wgqA8YQVAOMJKwDGE1YAjCesABhPWAEwnoVszxCrcFn7zcYAsBUzKwDGE1YAjCesABhPWAEw3rYaLKrqkSTfTfKDJM9298GquijJJ5NcmuSRJDd291O7UyYA6+xUZlY/191XdvfBxfbNSe7q7suT3LXYBoCl28nHgNclObq4fzTJ9TsvBwBebLth1Un+pKruq6rDi30Huvvxxf0nkhxYenUAkO3/KPgd3f1YVf29JHdW1VdPfLC7u6r6ZE9chNvhJDk/L9tRsQCsp23NrLr7scXt8SSfSvLWJE9W1cVJsrg9vslzj3T3we4+eE7OW07VAKyVLcOqqi6oqp987n6Sn0/y5SR3JDm0OOxQktt3q0gA1tt2PgY8kORTVfXc8X/Q3Z+tqi8kua2qbkryzSQ37l6ZAKyzLcOqux9OcsVJ9v+fJFfvRlEAcCIrWAAwnrACYDxhBcB4wgqA8Vwp+Azn6rvAOjCzAmA8M6sdurOP7XcJACvPzAqA8cysTtM1dcN+lwCwNsysABhPWAEwnrACYDxhBcB4wgqA8YQVAOMJKwDGE1YAjCesABhPWAEwnrACYDxhBcB4wgqA8YQVAOMJKwDGE1YAjCesABhPWAEwnrACYDxhBcB4wgqA8YQVAOMJKwDGE1YAjCesABhPWAEwnrACYDxhBcB4wgqA8YQVAOMJKwDGE1YAjCesABhPWAEwnrACYDxhBcB4wgqA8YQVAOMJKwDGE1YAjCesABhPWAEwnrACYLxthVVVXVhVx6rqq1X1UFW9raouqqo7q+rri9tX7HaxAKyn7c6sbkny2e5+Q5IrkjyU5OYkd3X35UnuWmwDwNJtGVZV9fIkP5PkY0nS3d/r7qeTXJfk6OKwo0mu360iAVhv25lZXZbk20l+r6rur6qPVtUFSQ509+OLY55IcuBkT66qw1V1b1Xd+/08s5yqAVgr2wmrs5O8JclHuvvNSf42L/jIr7s7SZ/syd19pLsPdvfBc3LeTusFYA1tJ6weTfJod9+z2D6WjfB6sqouTpLF7fHdKRGAdbdlWHX3E0m+VVWvX+y6OsmDSe5Icmix71CS23elQgDW3tnbPO5fJ/lEVZ2b5OEkv5KNoLutqm5K8s0kN+5OiQCsu22FVXc/kOTgSR66ernlAMCLWcECgPGEFQDjCSsAxhNWAIwnrAAYT1gBMJ6wAmA8YQXAeMIKgPGEFQDjCSsAxhNWAIwnrAAYT1gBMJ6wAmA8YQXAeNXde3eyqm9n46rCSfKqJN/Zs5PPYMzrwZhX37qNN9m9Mf/D7n71VgftaVg978RV93b3ya4+vLKMeT0Y8+pbt/Em+z9mHwMCMJ6wAmC8/QyrI/t47v1izOvBmFffuo032ecx79t3VgCwXT4GBGA8YQXAeHseVlV1bVV9raq+UVU37/X590pVfbyqjlfVl0/Yd1FV3VlVX1/cvmI/a1ymqnptVd1dVQ9W1Veq6n2L/as85vOr6n9U1f9cjPnfL/ZfVlX3LN7jn6yqc/e71mWrqrOq6v6q+vRie6XHXFWPVNWXquqBqrp3sW9l39tJUlUXVtWxqvpqVT1UVW/bzzHvaVhV1VlJfjfJP0/yxiS/WFVv3Msa9tDvJ7n2BftuTnJXd1+e5K7F9qp4Nsmvdvcbk1yV5L2Lf7erPOZnkryzu69IcmWSa6vqqiQfSvLh7n5dkqeS3LSPNe6W9yV56ITtdRjzz3X3lSf81miV39tJckuSz3b3G5JckY1/3/s35u7esz9J3pbkcydsfyDJB/ayhj0e76VJvnzC9teSXLy4f3GSr+13jbs49tuTXLMuY07ysiR/meSns/Er/7MX+5/3nl+FP0kuycb/qN6Z5NNJag3G/EiSV71g38q+t5O8PMn/yqIJb8KY9/pjwNck+dYJ248u9q2LA939+OL+E0kO7Gcxu6WqLk3y5iT3ZMXHvPg47IEkx5PcmeSvkjzd3c8uDlnF9/jvJPm1JH+32H5lVn/MneRPquq+qjq82LfK7+3Lknw7ye8tPu79aFVdkH0cswaLfdIbfzVZud8NVNVPJPmjJO/v7r858bFVHHN3/6C7r8zGbOOtSd6wzyXtqqp6d5Lj3X3ffteyx97R3W/JxlcY762qnznxwRV8b5+d5C1JPtLdb07yt3nBR357Pea9DqvHkrz2hO1LFvvWxZNVdXGSLG6P73M9S1VV52QjqD7R3X+82L3SY35Odz+d5O5sfAR2YVWdvXho1d7jb0/yC1X1SJJbs/FR4C1Z7TGnux9b3B5P8qls/MVkld/bjyZ5tLvvWWwfy0Z47duY9zqsvpDk8kXn0LlJ3pPkjj2uYT/dkeTQ4v6hbHyvsxKqqpJ8LMlD3f3bJzy0ymN+dVVduLj/49n4ju6hbITWDYvDVmrM3f2B7r6kuy/Nxn+/f9rdv5QVHnNVXVBVP/nc/SQ/n+TLWeH3dnc/keRbVfX6xa6rkzyYfRzznq9gUVXvysZn3mcl+Xh3/9aeFrBHquoPk/xsNpbVfzLJbyT5L0luS/IPsnGplBu7+6/3q8Zlqqp3JPlvSb6UH32X8evZ+N5qVcf8piRHs/Fe/rEkt3X3f6iqf5SNWcdFSe5P8i+7+5n9q3R3VNXPJvm33f3uVR7zYmyfWmyeneQPuvu3quqVWdH3dpJU1ZVJPprk3CQPJ/mVLN7n2YcxW24JgPE0WAAwnrACYDxhBcB4wgqA8YQVAOMJKwDGE1YAjPf/AeAsCQx3m1vCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110d02fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class CNN:\n",
    "\n",
    "    def __init__(self, trainloader, testloader):\n",
    "        self.trainloader = trainloader\n",
    "        self.testloader = testloader\n",
    "\n",
    "        self.net = Net()\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def run(self, epochs, learning_rate, momentum):\n",
    "\n",
    "        test_results = []\n",
    "\n",
    "        for epoch in range(1, epochs):\n",
    "            self.train(epoch, learning_rate, momentum, print_train_statistics=False)\n",
    "            result = self.test(epoch, print_test_statistics=True)\n",
    "\n",
    "            test_results.append(result)\n",
    "\n",
    "        best_results = max(test_results, key=lambda t: t[3])\n",
    "\n",
    "        print('Maximum Accuracy: {:.0f}%, average loss: {}, at epoch {}, correct: {}'.format(\n",
    "            best_results[3] * 100.,\n",
    "            best_results[1],\n",
    "            best_results[0],\n",
    "            best_results[2]\n",
    "        ))\n",
    "\n",
    "    def train(self, epoch, learning_rate, momentum, print_train_statistics=False):\n",
    "        optimizer = optim.SGD(\n",
    "            self.net.parameters(),\n",
    "            lr=learning_rate,\n",
    "            momentum=momentum\n",
    "        )\n",
    "\n",
    "        running_loss = 0.0\n",
    "        self.net.train()\n",
    "\n",
    "        for batch_index, (data, target) in enumerate(self.trainloader):\n",
    "            data, target = Variable(data), Variable(target).type(torch.LongTensor)\n",
    "            optimizer.zero_grad()\n",
    "            output = self.net(data)\n",
    "\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            if print_train_statistics:\n",
    "                running_loss += loss.data[0]\n",
    "                if batch_index % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                    print('[%d, %5d] loss: %.3f' % (epoch + 1, batch_index + 1, running_loss / 2000))\n",
    "                    running_loss = 0.0\n",
    "\n",
    "    def kaggle_train(self, epochs, learning_rate, momentum):\n",
    "        for epoch in range(1, epochs):\n",
    "            self.train(epoch, learning_rate, momentum,\n",
    "                       print_train_statistics=True)\n",
    "\n",
    "    def kaggle_test(self, epoch, print_test_statistics=False, kaggle=False):\n",
    "        self.net.eval()\n",
    "\n",
    "        predicted_labels = np.zeros(\n",
    "            len(self.testloader)*self.testloader.batch_size\n",
    "        )\n",
    "\n",
    "        for index, data in enumerate(self.testloader):\n",
    "\n",
    "            data = Variable(data, volatile=True)\n",
    "            output = self.net(data)\n",
    "\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "\n",
    "            batch_size = self.testloader.batch_size\n",
    "            batch_label = pred.numpy().flatten()\n",
    "            predicted_labels[index*batch_size:(index+1)*batch_size] = batch_label\n",
    "\n",
    "        return predicted_labels\n",
    "\n",
    "    def test(self, epoch, print_test_statistics=False):\n",
    "        self.net.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "\n",
    "        predicted_labels = np.zeros(\n",
    "            len(self.testloader)*self.testloader.batch_size\n",
    "        )\n",
    "\n",
    "        for index, (data, target) in enumerate(self.testloader):\n",
    "\n",
    "            data = Variable(data, volatile=True)\n",
    "            target = Variable(target).type(torch.LongTensor)\n",
    "            output = self.net(data)\n",
    "\n",
    "            # sum up batch loss\n",
    "            test_loss += F.nll_loss(\n",
    "                output,\n",
    "                target,\n",
    "                size_average=False\n",
    "            ).data[0]\n",
    "\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "\n",
    "            batch_size = self.testloader.batch_size\n",
    "            batch_label = pred.numpy().flatten()\n",
    "            predicted_labels[index*batch_size:(index+1)*batch_size] = batch_label\n",
    "\n",
    "            correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "        test_loss /= len(self.testloader.dataset)\n",
    "\n",
    "        if print_test_statistics:\n",
    "            print('Epoch: {} (Test Set) :Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "                epoch,\n",
    "                test_loss,\n",
    "                correct,\n",
    "                len(self.testloader.dataset),\n",
    "                100. * correct / len(self.testloader.dataset)\n",
    "            ))\n",
    "\n",
    "        return epoch, test_loss, correct, correct/len(self.testloader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'largest_digit_zoom_reshaped' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-f94ddb057589>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0miiii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlargest_digit_zoom_reshaped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlargest_digit_zoom_reshaped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'largest_digit_zoom_reshaped' is not defined"
     ]
    }
   ],
   "source": [
    "def _interal_testing():\n",
    "    mean = 0.5\n",
    "    std_dev = 0.5\n",
    "\n",
    "    trainloader, testloader = ImageGetter(\n",
    "        dataset_modifier='_int',\n",
    "        transform=True,\n",
    "        augment=False\n",
    "    ).process(mean, std_dev)\n",
    "\n",
    "    learning_rate = 0.0012\n",
    "    momentum = 0.9\n",
    "\n",
    "    cnn = CNN(trainloader, testloader).run(10, learning_rate, momentum)\n",
    "\n",
    "def _go_time():\n",
    "    mean = 0.5\n",
    "    std_dev = 0.5\n",
    "\n",
    "    trainloader, testloader = ImageGetter(\n",
    "        as_image=True,\n",
    "        transform=True,\n",
    "        augment=False\n",
    "    ).kaggle(mean, std_dev)\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    momentum = 0.9\n",
    "\n",
    "    cnn = CNN(trainloader, testloader)\n",
    "    cnn.kaggle_train(10, learning_rate, momentum)\n",
    "    predictions = cnn.kaggle_test(10, print_test_statistics=True)\n",
    "\n",
    "    print('writing data to file...')\n",
    "    dt = pd.DataFrame(data=predictions)\n",
    "    dt.columns = [\"Label\"]\n",
    "    dt = dt.astype(int)\n",
    "    dt.to_csv('../output/cnn_test.csv', mode='w',index=True, index_label='Id')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    _interal_testing()\n",
    "    #_go_time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(21, 17)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-4f1612c39a99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mcontours\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhierarchy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindContours\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthresh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminAreaRect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(10, 10, 5)\n",
    "        self.fc1 = nn.Linear(10*13*13, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        \n",
    "        conv = self.conv1(x)\n",
    "        pool = self.pool(conv)\n",
    "        x = F.relu(pool)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        conv = self.conv2(x)\n",
    "        pool = self.pool(conv)\n",
    "        x = F.relu(pool)\n",
    "\n",
    "        #print(x.shape)\n",
    "        # 4 is the batch size\n",
    "        x = x.view(-1, 10*13*13)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "def train(epoch, print_statistics):\n",
    "    running_loss = 0\n",
    "    net.train()\n",
    "    \n",
    "    for batch_index, (data, target) in enumerate(trainloader):\n",
    "        data, target = Variable(data), Variable(target).type(torch.LongTensor)\n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        if print_statistics:\n",
    "            running_loss += loss.data[0]\n",
    "            if batch_index % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' % (epoch + 1, batch_index + 1, running_loss / 2000))\n",
    "                running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for data, target in testloader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target).type(torch.LongTensor)\n",
    "        output = net(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "    test_loss /= len(testloader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(testloader.dataset), 100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.2070, Accuracy: 1671/10000 (17%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.9861, Accuracy: 2771/10000 (28%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.7830, Accuracy: 3703/10000 (37%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.7240, Accuracy: 3942/10000 (39%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.5044, Accuracy: 4958/10000 (50%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.4961, Accuracy: 4921/10000 (49%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.4050, Accuracy: 5308/10000 (53%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.3515, Accuracy: 5462/10000 (55%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.3785, Accuracy: 5407/10000 (54%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 10):\n",
    "    train(epoch, False)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
