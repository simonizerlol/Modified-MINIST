{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning cross validation separation...\n",
      "finished cross validation separation\n"
     ]
    }
   ],
   "source": [
    "from getdata import CrossValidation\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cv = CrossValidation(as_image=False)\n",
    "X, y, valid_x, valid_y = cv.get_set()\n",
    "\n",
    "#binarize\n",
    "X_c = np.array([[255 if x > 250 else 0 for x in y] for y in X])/255.0\n",
    "valid_x = np.array([[255 if x > 250 else 0 for x in y] for y in valid_x])/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X_c[0].reshape((64,64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.29225561\n",
      "0.1555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 2.22297770\n",
      "Iteration 3, loss = 2.16618011\n",
      "Iteration 4, loss = 2.11518689\n",
      "Iteration 5, loss = 2.06984257\n",
      "Iteration 6, loss = 2.02756448\n",
      "Iteration 7, loss = 1.98889708\n",
      "Iteration 8, loss = 1.95263193\n",
      "Iteration 9, loss = 1.91787931\n",
      "Iteration 10, loss = 1.88345200\n",
      "Iteration 11, loss = 1.84909281\n",
      "0.2277\n",
      "Iteration 12, loss = 1.81597277\n",
      "Iteration 13, loss = 1.78413757\n",
      "Iteration 14, loss = 1.75289003\n",
      "Iteration 15, loss = 1.72241113\n",
      "Iteration 16, loss = 1.69287809\n",
      "Iteration 17, loss = 1.66311830\n",
      "Iteration 18, loss = 1.63386990\n",
      "Iteration 19, loss = 1.60518935\n",
      "Iteration 20, loss = 1.57634331\n",
      "Iteration 21, loss = 1.54832904\n",
      "0.2176\n",
      "Iteration 22, loss = 1.52023274\n",
      "Iteration 23, loss = 1.49265397\n",
      "Iteration 24, loss = 1.46520215\n",
      "Iteration 25, loss = 1.43827649\n",
      "Iteration 26, loss = 1.41187840\n",
      "Iteration 27, loss = 1.38542230\n",
      "Iteration 28, loss = 1.35918125\n",
      "Iteration 29, loss = 1.33355076\n",
      "Iteration 30, loss = 1.30810312\n",
      "Iteration 31, loss = 1.28276903\n",
      "0.2037\n",
      "Iteration 32, loss = 1.25757759\n",
      "Iteration 33, loss = 1.23296721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34, loss = 1.20174857\n",
      "Iteration 35, loss = 1.17593527\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-59aab3cc9bf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                     learning_rate_init=.01, warm_start = True)\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    971\u001b[0m         \"\"\"\n\u001b[1;32m    972\u001b[0m         return self._fit(X, y, incremental=(self.warm_start and\n\u001b[0;32m--> 973\u001b[0;31m                                             hasattr(self, \"classes_\")))\n\u001b[0m\u001b[1;32m    974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#baseline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=43, max_iter=1, alpha=1e-2,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.01, warm_start = True)\n",
    "for i in range(100):\n",
    "    clf.fit(X_c, y)\n",
    "    if i % 10 == 0:\n",
    "        print(clf.score(valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit, logit\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import sys\n",
    "class NNClassifier:\n",
    "\n",
    "    def __init__(self, n_classes, n_features, n_hidden_units=30,\n",
    "                 epochs=500, lr=0.01, l2 = 0.001):\n",
    "        np.random.seed(1)\n",
    "        self.n_classes = n_classes\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        self.w_hidden, self.w_out = self.weights()\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = lr\n",
    "        self.l2 = l2\n",
    "\n",
    "    def weights(self):\n",
    "        w_hidden = np.random.uniform(-1.0, 1.0, size=self.n_hidden_units * (self.n_features + 1))\n",
    "        w_hidden = w_hidden.reshape(self.n_hidden_units, self.n_features + 1)\n",
    "        w_out = np.random.uniform(-1.0, 1.0, \n",
    "                               size=self.n_classes * (self.n_hidden_units + 1))\n",
    "        w_out = w_out.reshape(self.n_classes, self.n_hidden_units + 1)\n",
    "        return w_hidden, w_out\n",
    "\n",
    "    def add_bias_unit(self, X, column=True):\n",
    "        if column:\n",
    "            bias_added = np.ones((X.shape[0], X.shape[1] + 1))\n",
    "            bias_added[:, 1:] = X\n",
    "        else:\n",
    "            bias_added = np.ones((X.shape[0] + 1, X.shape[1]))\n",
    "            bias_added[1:, :] = X\n",
    "\n",
    "        return bias_added\n",
    "\n",
    "    def _forward(self, X):\n",
    "        nn_input = self.add_bias_unit(X, True)\n",
    "        nn_hidden = self.w_hidden.dot(nn_input.T)\n",
    "        act_hidden = self._sigmoid(nn_hidden)\n",
    "        act_hidden = self.add_bias_unit(act_hidden, False)\n",
    "        nn_out = self.w_out.dot(act_hidden)\n",
    "        act_out = self._sigmoid(nn_out)\n",
    "        return nn_input, nn_hidden, act_hidden, nn_out, act_out\n",
    "    \n",
    "    def _backward(self, nn_input, nn_hidden, act_hidden, act_out, y):\n",
    "        s3 = act_out - y\n",
    "        nn_hidden = self.add_bias_unit(nn_hidden, column=False)\n",
    "        s2 = self.w_out.T.dot(s3) * self._sigmoid_prime(nn_hidden)\n",
    "        s2 = s2[1:, :]\n",
    "        g1 = s2.dot(nn_input)\n",
    "        g2 = s3.dot(act_hidden.T)\n",
    "        return g1, g2\n",
    "\n",
    "    def _backpropogate(self, X, y):\n",
    "        nn_input, nn_hidden, act_hidden, nn_out, act_out = self._forward(X)\n",
    "\n",
    "        g1, g2 = self._backward(nn_input, nn_hidden, act_hidden, act_out, y)\n",
    "\n",
    "        error = self._error(y.T, act_out)\n",
    "        g1[:, 1:]+= (self.w_hidden[:, 1:]*self.l2) \n",
    "        g2[:, 1:]+= (self.w_out[:, 1:]*self.l2)\n",
    "        return error, g1, g2\n",
    "    \n",
    "    def _cross_entropy(self, y, out):\n",
    "        log_loss = - np.sum(y*np.log(out))\n",
    "        return log_loss/y.shape[1]\n",
    "    \n",
    "    def _error(self, y, out):\n",
    "        l2_term = (self.l2/2.0) * (np.sum(np.square(self.w_hidden[:, 1:])) + np.sum(np.square(self.w_out[:, 1:])))\n",
    "        return self._cross_entropy(y, self._smax(out.T)) + l2_term\n",
    "    \n",
    "    def _sigmoid_prime(self, x):\n",
    "        return self._sigmoid(x) * (1 - self._sigmoid(x)) \n",
    "    \n",
    "    def _sigmoid(self, x):\n",
    "        return expit(x)\n",
    "    \n",
    "    def _one_hot(self, y_data, n_classes):\n",
    "        oh = np.zeros((len(y_data), n_classes))\n",
    "        oh[np.arange(len(y_data)), y_data] = 1\n",
    "        return oh\n",
    "    \n",
    "    def _smax(self, X):\n",
    "        logC = -np.max(X)\n",
    "        return np.exp(X + logC)/np.sum(np.exp(X + logC), axis = 0)\n",
    "    \n",
    "    def _mle(self, out):\n",
    "        return [np.argmax(x) for x in self._smax(out)]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_c = X.copy()\n",
    "        nn_input, nn_hidden, act_hidden, nn_out, act_out = self._forward(X_c)\n",
    "        return self._mle(nn_out.T)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X_c = X.copy()\n",
    "        nn_input, nn_hidden, act_hidden, nn_out, act_out = self._forward(X_c)\n",
    "        return self._smax(act_out.T)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.error_ = []\n",
    "        y_data_enc = self._one_hot(y, self.n_classes).T\n",
    "        train_scores = []\n",
    "        valid_scores = []\n",
    "        for i in range(self.epochs):   \n",
    "            # update weights\n",
    "            error, g1, g2 = self._backpropogate(X, y_data_enc)\n",
    "            self.error_.append(error)\n",
    "            print('Iteration {}, loss = {}, valid_score=  {}'.format(i, error, self.score(valid_x, valid_y)))\n",
    "            self.w_hidden -= (self.learning_rate * g1)\n",
    "            self.w_out -= (self.learning_rate * g2)\n",
    "        return self\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_hat = self.predict(X)\n",
    "        return np.sum(y == y_hat, axis=0) / float(X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_hidden_units = np.arange(30, 40)\n",
    "lr = [0.00001, 0.0005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5]\n",
    "l2 = np.arange(0.0001, 0.0009, 0.0001)\n",
    "X_s = X_c[:1000]\n",
    "y_s = y[:1000]\n",
    "print(lr)\n",
    "unit = np.random.choice(n_hidden_units)\n",
    "learn_rate = np.random.choice(lr)\n",
    "reg = np.random.choice(l2)\n",
    "f1_history_lr = []\n",
    "for i in lr:\n",
    "    nn = NNClassifier(n_classes=10, \n",
    "                  n_features=4096,\n",
    "                  n_hidden_units=40,\n",
    "                  epochs=100,\n",
    "                  lr=i, l2=0.001)\n",
    "    print('[INFO]    Units: {}'.format(40))\n",
    "    print('[INFO]    Learning Rate: {}'.format(i))\n",
    "    print('[INFO]    L2 Regularization: {}'.format(reg))\n",
    "    nn = nn.fit(X_s, y_s)\n",
    "    f1_history_lr.append((nn, f1_score(valid_y, nn.predict(valid_x), average = 'macro')))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "f1_scores_lr = [n[1] for n in f1_history_lr]\n",
    "print(lr[np.argmax(f1_scores_lr)])\n",
    "plt.plot(lr, f1_scores_lr)\n",
    "print(np.ptp(f1_scores_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regularization Tuning\n",
    "\n",
    "n_hidden_units = np.arange(30, 40)\n",
    "l2 = [0.001, 0.005, 0.01, 0.05, 0.1, 1, 10, 100]\n",
    "print(lr)\n",
    "unit = np.random.choice(n_hidden_units)\n",
    "learn_rate = np.random.choice(lr)\n",
    "reg = np.random.choice(l2)\n",
    "f1_history_l2 = []\n",
    "for i in l2:\n",
    "    nn = NNClassifier(n_classes=10, \n",
    "                  n_features=4096,\n",
    "                  n_hidden_units=40,\n",
    "                  epochs=100,\n",
    "                  lr=lr[np.argmax(f1_scores_lr)], l2=i)\n",
    "    print('[INFO]    Units: {}'.format(40))\n",
    "    print('[INFO]    Learning Rate: {}'.format(lr[np.argmax(f1_scores_lr)], l2=i))\n",
    "    print('[INFO]    L2 Regularization: {}'.format(i))\n",
    "    nn = nn.fit(X_s, y_s)\n",
    "    f1_history_l2.append((nn, f1_score(valid_y, nn.predict(valid_x), average = 'macro')))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "f1_scores_l2 = [n[1] for n in f1_history_l2]\n",
    "print(f1_scores_l2)\n",
    "plt.plot(lr, f1_scores_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#units Tuning\n",
    "\n",
    "n_hidden_units = np.arange(10, 70)\n",
    "print(n_hidden_units)\n",
    "f1_history_u = []\n",
    "for i in n_hidden_units:\n",
    "    nn = NNClassifier(n_classes=10, \n",
    "                  n_features=4096,\n",
    "                  n_hidden_units=i,\n",
    "                  epochs=100,\n",
    "                  lr=lr[np.argmax(f1_scores_lr)], l2=l2[np.argmax(f1_scores_l2)])\n",
    "    print('[INFO]    Units: {}'.format(i))\n",
    "    print('[INFO]    Learning Rate: {}'.format(lr[np.argmax(f1_scores_lr)], l2=i))\n",
    "    print('[INFO]    L2 Regularization: {}'.format(l2[np.argmax(f1_scores_l2)]))\n",
    "    nn = nn.fit(X_s, y_s)\n",
    "    f1_history_u.append((nn, f1_score(valid_y, nn.predict(valid_x), average = 'macro')))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "f1_scores_u = [n[1] for n in f1_history_u]\n",
    "print(f1_scores_u)\n",
    "plt.plot(n_hidden_units, f1_scores_u)\n",
    "print('\\n{}'.format(n_hidden_units[np.argmax(f1_scores_u)]))\n",
    "print(np.amax(f1_scores_u))\n",
    "print(np.ptp(f1_scores_u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 1416726.2484588146, valid_score=  0.1009\n",
      "Iteration 1, loss = 1395697.7516761625, valid_score=  0.1043\n",
      "Iteration 2, loss = 1366728.9141277857, valid_score=  0.1061\n",
      "Iteration 3, loss = 1339541.3130826913, valid_score=  0.1037\n",
      "Iteration 4, loss = 1313308.8741692393, valid_score=  0.1037\n",
      "Iteration 5, loss = 1287750.5342154412, valid_score=  0.1038\n",
      "Iteration 6, loss = 1262759.5594741402, valid_score=  0.1038\n",
      "Iteration 7, loss = 1238295.29136828, valid_score=  0.1064\n",
      "Iteration 8, loss = 1214338.0968658552, valid_score=  0.1052\n",
      "Iteration 9, loss = 1190873.9534195492, valid_score=  0.1055\n",
      "Iteration 10, loss = 1167890.7291667056, valid_score=  0.1063\n",
      "Iteration 11, loss = 1145377.181654912, valid_score=  0.1059\n",
      "Iteration 12, loss = 1123322.6754312562, valid_score=  0.1066\n",
      "Iteration 13, loss = 1101716.900827746, valid_score=  0.1073\n",
      "Iteration 14, loss = 1080549.8733123483, valid_score=  0.1087\n",
      "Iteration 15, loss = 1059811.9443021212, valid_score=  0.1092\n",
      "Iteration 16, loss = 1039493.784918814, valid_score=  0.1106\n",
      "Iteration 17, loss = 1019586.3522893689, valid_score=  0.1124\n",
      "Iteration 18, loss = 1000080.8514697052, valid_score=  0.1139\n",
      "Iteration 19, loss = 980968.7145759242, valid_score=  0.114\n",
      "Iteration 20, loss = 962241.5935380453, valid_score=  0.1152\n",
      "Iteration 21, loss = 943891.352290717, valid_score=  0.1145\n",
      "Iteration 22, loss = 925910.0697135945, valid_score=  0.1148\n",
      "Iteration 23, loss = 908290.0337818627, valid_score=  0.1149\n",
      "Iteration 24, loss = 891023.729538208, valid_score=  0.1142\n",
      "Iteration 25, loss = 874103.8283185406, valid_score=  0.1149\n",
      "Iteration 26, loss = 857523.1798056039, valid_score=  0.1155\n",
      "Iteration 27, loss = 841274.8051805553, valid_score=  0.1155\n",
      "Iteration 28, loss = 825351.8899089746, valid_score=  0.1159\n",
      "Iteration 29, loss = 809747.777352475, valid_score=  0.1173\n",
      "Iteration 30, loss = 794455.9630659168, valid_score=  0.1181\n",
      "Iteration 31, loss = 779470.0892812145, valid_score=  0.1182\n",
      "Iteration 32, loss = 764783.9396287883, valid_score=  0.119\n",
      "Iteration 33, loss = 750391.434263147, valid_score=  0.1201\n",
      "Iteration 34, loss = 736286.6254396185, valid_score=  0.1208\n",
      "Iteration 35, loss = 722463.6934986743, valid_score=  0.1209\n",
      "Iteration 36, loss = 708916.9431881683, valid_score=  0.1203\n",
      "Iteration 37, loss = 695640.8002365149, valid_score=  0.1205\n",
      "Iteration 38, loss = 682629.8081010703, valid_score=  0.1217\n",
      "Iteration 39, loss = 669878.6248520535, valid_score=  0.1222\n",
      "Iteration 40, loss = 657382.0201823396, valid_score=  0.123\n",
      "Iteration 41, loss = 645134.8725532622, valid_score=  0.1232\n",
      "Iteration 42, loss = 633132.1664972546, valid_score=  0.1243\n",
      "Iteration 43, loss = 621368.9900899436, valid_score=  0.1247\n",
      "Iteration 44, loss = 609840.5325808597, valid_score=  0.1245\n",
      "Iteration 45, loss = 598542.0821517042, valid_score=  0.1252\n",
      "Iteration 46, loss = 587469.0237659103, valid_score=  0.1247\n",
      "Iteration 47, loss = 576616.8370808044, valid_score=  0.125\n",
      "Iteration 48, loss = 565981.0944057137, valid_score=  0.1253\n",
      "Iteration 49, loss = 555557.4586993203, valid_score=  0.1259\n",
      "Iteration 50, loss = 545341.6816050175, valid_score=  0.1263\n",
      "Iteration 51, loss = 535329.6015246544, valid_score=  0.126\n",
      "Iteration 52, loss = 525517.1417309599, valid_score=  0.127\n",
      "Iteration 53, loss = 515900.30851871235, valid_score=  0.1276\n",
      "Iteration 54, loss = 506475.1893950423, valid_score=  0.1281\n",
      "Iteration 55, loss = 497237.9513098868, valid_score=  0.1285\n",
      "Iteration 56, loss = 488184.8389282225, valid_score=  0.1289\n",
      "Iteration 57, loss = 479312.1729459286, valid_score=  0.1296\n",
      "Iteration 58, loss = 470616.3484509343, valid_score=  0.1293\n",
      "Iteration 59, loss = 462093.8333306697, valid_score=  0.1298\n",
      "Iteration 60, loss = 453741.1667259171, valid_score=  0.1301\n",
      "Iteration 61, loss = 445554.9575300753, valid_score=  0.1302\n",
      "Iteration 62, loss = 437531.88293179777, valid_score=  0.1311\n",
      "Iteration 63, loss = 429668.68699807697, valid_score=  0.1321\n",
      "Iteration 64, loss = 421962.179294229, valid_score=  0.1327\n",
      "Iteration 65, loss = 414409.23353698535, valid_score=  0.1337\n",
      "Iteration 66, loss = 407006.7862770556, valid_score=  0.134\n",
      "Iteration 67, loss = 399751.8356081563, valid_score=  0.1347\n",
      "Iteration 68, loss = 392641.4399005805, valid_score=  0.1355\n",
      "Iteration 69, loss = 385672.71655879705, valid_score=  0.1354\n",
      "Iteration 70, loss = 378842.8408040192, valid_score=  0.1357\n",
      "Iteration 71, loss = 372149.04448374774, valid_score=  0.1365\n",
      "Iteration 72, loss = 365588.61491060856, valid_score=  0.1361\n",
      "Iteration 73, loss = 359158.8937322085, valid_score=  0.1376\n",
      "Iteration 74, loss = 352857.27583243477, valid_score=  0.1381\n",
      "Iteration 75, loss = 346681.2082630912, valid_score=  0.1382\n",
      "Iteration 76, loss = 340628.18920353614, valid_score=  0.1377\n",
      "Iteration 77, loss = 334695.7669453996, valid_score=  0.1379\n",
      "Iteration 78, loss = 328881.53889959416, valid_score=  0.1378\n",
      "Iteration 79, loss = 323183.1506234683, valid_score=  0.1385\n",
      "Iteration 80, loss = 317598.29486678384, valid_score=  0.1396\n",
      "Iteration 81, loss = 312124.7106359167, valid_score=  0.1398\n",
      "Iteration 82, loss = 306760.1822760765, valid_score=  0.1402\n",
      "Iteration 83, loss = 301502.5385713946, valid_score=  0.141\n",
      "Iteration 84, loss = 296349.6518625071, valid_score=  0.1415\n",
      "Iteration 85, loss = 291299.437180938, valid_score=  0.1416\n",
      "Iteration 86, loss = 286349.8513993165, valid_score=  0.1422\n",
      "Iteration 87, loss = 281498.892396347, valid_score=  0.1428\n",
      "Iteration 88, loss = 276744.5982355004, valid_score=  0.1431\n",
      "Iteration 89, loss = 272085.046356552, valid_score=  0.1429\n",
      "Iteration 90, loss = 267518.3527792721, valid_score=  0.1439\n",
      "Iteration 91, loss = 263042.6713187094, valid_score=  0.144\n",
      "Iteration 92, loss = 258656.19281156868, valid_score=  0.1443\n",
      "Iteration 93, loss = 254357.14435321782, valid_score=  0.1449\n",
      "Iteration 94, loss = 250143.78854493238, valid_score=  0.1441\n",
      "Iteration 95, loss = 246014.42275121395, valid_score=  0.1447\n",
      "Iteration 96, loss = 241967.37836744258, valid_score=  0.146\n",
      "Iteration 97, loss = 238001.02009874297, valid_score=  0.1468\n",
      "Iteration 98, loss = 234113.74525164286, valid_score=  0.1471\n",
      "Iteration 99, loss = 230303.98304069036, valid_score=  0.147\n",
      "Iteration 100, loss = 226570.19391247706, valid_score=  0.1478\n",
      "Iteration 101, loss = 222910.8688893381, valid_score=  0.148\n",
      "Iteration 102, loss = 219324.52893437084, valid_score=  0.1486\n",
      "Iteration 103, loss = 215809.72433846424, valid_score=  0.1495\n",
      "Iteration 104, loss = 212365.03412898773, valid_score=  0.1493\n",
      "Iteration 105, loss = 208989.06549887647, valid_score=  0.1493\n",
      "Iteration 106, loss = 205680.45325420328, valid_score=  0.1496\n",
      "Iteration 107, loss = 202437.85927798937, valid_score=  0.1505\n",
      "Iteration 108, loss = 199259.97200792434, valid_score=  0.151\n",
      "Iteration 109, loss = 196145.5059257716, valid_score=  0.1509\n",
      "Iteration 110, loss = 193093.2010564614, valid_score=  0.1509\n",
      "Iteration 111, loss = 190101.82247516475, valid_score=  0.1516\n",
      "Iteration 112, loss = 187170.15982099448, valid_score=  0.1527\n",
      "Iteration 113, loss = 184297.0268163653, valid_score=  0.1516\n",
      "Iteration 114, loss = 181481.26079146645, valid_score=  0.153\n",
      "Iteration 115, loss = 178721.72221373254, valid_score=  0.1531\n",
      "Iteration 116, loss = 176017.29422261647, valid_score=  0.1533\n",
      "Iteration 117, loss = 173366.8821703315, valid_score=  0.1544\n",
      "Iteration 118, loss = 170769.4131695007, valid_score=  0.1549\n",
      "Iteration 119, loss = 168223.8356487993, valid_score=  0.1551\n",
      "Iteration 120, loss = 165729.1189176756, valid_score=  0.1559\n",
      "Iteration 121, loss = 163284.2527410991, valid_score=  0.1559\n",
      "Iteration 122, loss = 160888.24692502635, valid_score=  0.1562\n",
      "Iteration 123, loss = 158540.13091294575, valid_score=  0.1573\n",
      "Iteration 124, loss = 156238.95339350778, valid_score=  0.1582\n",
      "Iteration 125, loss = 153983.78191891796, valid_score=  0.1589\n",
      "Iteration 126, loss = 151773.70253350635, valid_score=  0.1596\n",
      "Iteration 127, loss = 149607.81941170918, valid_score=  0.1592\n",
      "Iteration 128, loss = 147485.2545046154, valid_score=  0.1602\n",
      "Iteration 129, loss = 145405.14719423116, valid_score=  0.1611\n",
      "Iteration 130, loss = 143366.65395468022, valid_score=  0.1616\n",
      "Iteration 131, loss = 141368.94801967495, valid_score=  0.1624\n",
      "Iteration 132, loss = 139411.21905572276, valid_score=  0.1627\n",
      "Iteration 133, loss = 137492.67284067738, valid_score=  0.1627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 134, loss = 135612.53094737022, valid_score=  0.1643\n",
      "Iteration 135, loss = 133770.03043216994, valid_score=  0.1641\n",
      "Iteration 136, loss = 131964.42352840205, valid_score=  0.1636\n",
      "Iteration 137, loss = 130194.97734461854, valid_score=  0.1628\n",
      "Iteration 138, loss = 128460.97356774267, valid_score=  0.163\n",
      "Iteration 139, loss = 126761.70817112508, valid_score=  0.1631\n",
      "Iteration 140, loss = 125096.49112754705, valid_score=  0.1635\n",
      "Iteration 141, loss = 123464.64612719518, valid_score=  0.1644\n",
      "Iteration 142, loss = 121865.51030061906, valid_score=  0.1648\n",
      "Iteration 143, loss = 120298.43394667497, valid_score=  0.1649\n",
      "Iteration 144, loss = 118762.78026545588, valid_score=  0.1659\n",
      "Iteration 145, loss = 117257.9250962142, valid_score=  0.1662\n",
      "Iteration 146, loss = 115783.2566602983, valid_score=  0.166\n",
      "Iteration 147, loss = 114338.17530914024, valid_score=  0.1674\n",
      "Iteration 148, loss = 112922.09327734614, valid_score=  0.1685\n",
      "Iteration 149, loss = 111534.43444094872, valid_score=  0.1687\n",
      "Iteration 150, loss = 110174.63408087283, valid_score=  0.1697\n",
      "Iteration 151, loss = 108842.13865164365, valid_score=  0.1699\n",
      "Iteration 152, loss = 107536.40555533153, valid_score=  0.1704\n",
      "Iteration 153, loss = 106256.90292068318, valid_score=  0.1707\n",
      "Iteration 154, loss = 105003.10938734285, valid_score=  0.1713\n",
      "Iteration 155, loss = 103774.51389502855, valid_score=  0.1717\n",
      "Iteration 156, loss = 102570.61547750066, valid_score=  0.1728\n",
      "Iteration 157, loss = 101390.92306114704, valid_score=  0.1729\n",
      "Iteration 158, loss = 100234.95526801448, valid_score=  0.1729\n",
      "Iteration 159, loss = 99102.24022312528, valid_score=  0.1735\n",
      "Iteration 160, loss = 97992.31536594722, valid_score=  0.173\n",
      "Iteration 161, loss = 96904.72726589296, valid_score=  0.1727\n",
      "Iteration 162, loss = 95839.03144177338, valid_score=  0.1729\n",
      "Iteration 163, loss = 94794.7921850833, valid_score=  0.1728\n",
      "Iteration 164, loss = 93771.5823871308, valid_score=  0.1732\n",
      "Iteration 165, loss = 92768.98336974785, valid_score=  0.1737\n",
      "Iteration 166, loss = 91786.58471992463, valid_score=  0.1743\n",
      "Iteration 167, loss = 90823.98412731389, valid_score=  0.1746\n",
      "Iteration 168, loss = 89880.78722677391, valid_score=  0.1759\n",
      "Iteration 169, loss = 88956.60744045001, valid_score=  0.1755\n",
      "Iteration 170, loss = 88051.0658324298, valid_score=  0.1757\n",
      "Iteration 171, loss = 87163.79094325955, valid_score=  0.1762\n",
      "Iteration 172, loss = 86294.41868669503, valid_score=  0.1766\n",
      "Iteration 173, loss = 85442.59209598298, valid_score=  0.1769\n",
      "Iteration 174, loss = 84607.96147788977, valid_score=  0.1772\n",
      "Iteration 175, loss = 83790.18348110636, valid_score=  0.1782\n",
      "Iteration 176, loss = 82988.92314561235, valid_score=  0.1785\n",
      "Iteration 177, loss = 82203.84765973812, valid_score=  0.1791\n",
      "Iteration 178, loss = 81434.64362645784, valid_score=  0.1794\n",
      "Iteration 179, loss = 80680.9665718684, valid_score=  0.1794\n",
      "Iteration 180, loss = 79942.58911488467, valid_score=  0.1792\n",
      "Iteration 181, loss = 79218.95720174193, valid_score=  0.1796\n",
      "Iteration 182, loss = 78510.541071601, valid_score=  0.1797\n",
      "Iteration 183, loss = 77814.67644522217, valid_score=  0.1794\n",
      "Iteration 184, loss = 77138.7881505246, valid_score=  0.18\n",
      "Iteration 185, loss = 76460.06066900838, valid_score=  0.1778\n",
      "Iteration 186, loss = 75873.02415882598, valid_score=  0.1697\n",
      "Iteration 187, loss = 75167.0527089731, valid_score=  0.1767\n",
      "Iteration 188, loss = 75045.54530903984, valid_score=  0.1386\n",
      "Iteration 189, loss = 74324.42392609749, valid_score=  0.1491\n",
      "Iteration 190, loss = 73654.14352516379, valid_score=  0.1507\n",
      "Iteration 191, loss = 73015.83098800507, valid_score=  0.1511\n",
      "Iteration 192, loss = 72401.19146336484, valid_score=  0.1518\n",
      "Iteration 193, loss = 71802.8700742246, valid_score=  0.1514\n",
      "Iteration 194, loss = 71217.87850686056, valid_score=  0.1532\n",
      "Iteration 195, loss = 70645.43836324752, valid_score=  0.155\n",
      "Iteration 196, loss = 70085.25774747047, valid_score=  0.1586\n",
      "Iteration 197, loss = 69537.14084022399, valid_score=  0.1593\n",
      "Iteration 198, loss = 69000.90665013512, valid_score=  0.161\n",
      "Iteration 199, loss = 68476.37940613873, valid_score=  0.1649\n",
      "Iteration 200, loss = 67963.39515991157, valid_score=  0.1657\n",
      "Iteration 201, loss = 67461.78215916199, valid_score=  0.1667\n",
      "Iteration 202, loss = 66971.31834936567, valid_score=  0.1681\n",
      "Iteration 203, loss = 66491.71461904408, valid_score=  0.1704\n",
      "Iteration 204, loss = 66022.65615668049, valid_score=  0.172\n",
      "Iteration 205, loss = 65563.88118351727, valid_score=  0.1738\n",
      "Iteration 206, loss = 65115.25964450874, valid_score=  0.1756\n",
      "Iteration 207, loss = 64676.83940972469, valid_score=  0.1775\n",
      "Iteration 208, loss = 64248.78174193236, valid_score=  0.1795\n",
      "Iteration 209, loss = 63831.12745262364, valid_score=  0.1802\n",
      "Iteration 210, loss = 63423.60492445842, valid_score=  0.18\n",
      "Iteration 211, loss = 63025.72759329488, valid_score=  0.1802\n",
      "Iteration 212, loss = 62636.99973816148, valid_score=  0.1821\n",
      "Iteration 213, loss = 62257.05371132698, valid_score=  0.1817\n",
      "Iteration 214, loss = 61885.6337994367, valid_score=  0.182\n",
      "Iteration 215, loss = 61522.578323040856, valid_score=  0.1825\n",
      "Iteration 216, loss = 61167.67169015566, valid_score=  0.1823\n",
      "Iteration 217, loss = 60820.72863686418, valid_score=  0.1833\n",
      "Iteration 218, loss = 60481.3033750849, valid_score=  0.1827\n",
      "Iteration 219, loss = 60149.43985859591, valid_score=  0.1826\n",
      "Iteration 220, loss = 59824.05468438761, valid_score=  0.1825\n",
      "Iteration 221, loss = 59507.05914360098, valid_score=  0.1825\n",
      "Iteration 222, loss = 59192.65214859901, valid_score=  0.1818\n",
      "Iteration 223, loss = 58896.746900572165, valid_score=  0.1822\n",
      "Iteration 224, loss = 58577.6706951519, valid_score=  0.1828\n",
      "Iteration 225, loss = 58397.62218009515, valid_score=  0.1719\n",
      "Iteration 226, loss = 58009.34672068461, valid_score=  0.1839\n",
      "Iteration 227, loss = 57781.741788509025, valid_score=  0.1731\n",
      "Iteration 228, loss = 57437.10971684857, valid_score=  0.1835\n",
      "Iteration 229, loss = 57246.6742409656, valid_score=  0.1723\n",
      "Iteration 230, loss = 56906.74230760212, valid_score=  0.1861\n",
      "Iteration 231, loss = 56679.899698394336, valid_score=  0.1791\n",
      "Iteration 232, loss = 56379.33313488034, valid_score=  0.1851\n",
      "Iteration 233, loss = 56211.63783927125, valid_score=  0.1735\n",
      "Iteration 234, loss = 55893.15517314943, valid_score=  0.186\n",
      "Iteration 235, loss = 55690.204490882425, valid_score=  0.1793\n",
      "Iteration 236, loss = 55407.47902377542, valid_score=  0.1864\n",
      "Iteration 237, loss = 55256.189069133274, valid_score=  0.1749\n",
      "Iteration 238, loss = 54959.18530119928, valid_score=  0.1868\n",
      "Iteration 239, loss = 54777.276306059866, valid_score=  0.1798\n",
      "Iteration 240, loss = 54512.041446264484, valid_score=  0.1871\n",
      "Iteration 241, loss = 54376.50811267051, valid_score=  0.1761\n",
      "Iteration 242, loss = 54098.94925409587, valid_score=  0.1874\n",
      "Iteration 243, loss = 53936.27541014732, valid_score=  0.1795\n",
      "Iteration 244, loss = 53687.25566792551, valid_score=  0.1874\n",
      "Iteration 245, loss = 53566.61501451761, valid_score=  0.1768\n",
      "Iteration 246, loss = 53306.784548215655, valid_score=  0.1869\n",
      "Iteration 247, loss = 53161.41817310734, valid_score=  0.1806\n",
      "Iteration 248, loss = 52927.63730667194, valid_score=  0.1871\n",
      "Iteration 249, loss = 52820.88732375864, valid_score=  0.1768\n",
      "Iteration 250, loss = 52577.30816276705, valid_score=  0.1866\n",
      "Iteration 251, loss = 52447.85208775857, valid_score=  0.1808\n",
      "Iteration 252, loss = 52228.14657033667, valid_score=  0.1873\n",
      "Iteration 253, loss = 52134.29987584617, valid_score=  0.1777\n",
      "Iteration 254, loss = 51905.66672514793, valid_score=  0.1868\n",
      "Iteration 255, loss = 51790.72586806627, valid_score=  0.1812\n",
      "Iteration 256, loss = 51584.11078564695, valid_score=  0.1875\n",
      "Iteration 257, loss = 51502.19513881497, valid_score=  0.1794\n",
      "Iteration 258, loss = 51287.34165746488, valid_score=  0.1866\n",
      "Iteration 259, loss = 51185.73029427998, valid_score=  0.1816\n",
      "Iteration 260, loss = 50991.234391316044, valid_score=  0.1881\n",
      "Iteration 261, loss = 50920.30784221798, valid_score=  0.1799\n",
      "Iteration 262, loss = 50718.19323505855, valid_score=  0.1872\n",
      "Iteration 263, loss = 50628.81225257446, valid_score=  0.1817\n",
      "Iteration 264, loss = 50445.5534084556, valid_score=  0.1894\n",
      "Iteration 265, loss = 50384.71028129814, valid_score=  0.1801\n",
      "Iteration 266, loss = 50194.40245303386, valid_score=  0.188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 267, loss = 50116.26003580007, valid_score=  0.1823\n",
      "Iteration 268, loss = 49943.41815918022, valid_score=  0.1908\n",
      "Iteration 269, loss = 49891.788844791416, valid_score=  0.1808\n",
      "Iteration 270, loss = 49712.45484360485, valid_score=  0.1881\n",
      "Iteration 271, loss = 49644.64368948562, valid_score=  0.1837\n",
      "Iteration 272, loss = 49481.46615356227, valid_score=  0.1913\n",
      "Iteration 273, loss = 49438.219365359706, valid_score=  0.1824\n",
      "Iteration 274, loss = 49269.11430557536, valid_score=  0.189\n",
      "Iteration 275, loss = 49210.802984004884, valid_score=  0.1841\n",
      "Iteration 276, loss = 49056.600338377444, valid_score=  0.1913\n",
      "Iteration 277, loss = 49020.94621687973, valid_score=  0.1836\n",
      "Iteration 278, loss = 48861.40241028878, valid_score=  0.1894\n",
      "Iteration 279, loss = 48811.82343414999, valid_score=  0.1847\n",
      "Iteration 280, loss = 48665.96777752352, valid_score=  0.1914\n",
      "Iteration 281, loss = 48637.16410381057, valid_score=  0.1842\n",
      "Iteration 282, loss = 48486.57866183414, valid_score=  0.1909\n",
      "Iteration 283, loss = 48445.01759820991, valid_score=  0.1845\n",
      "Iteration 284, loss = 48306.940048085584, valid_score=  0.1915\n",
      "Iteration 285, loss = 48284.30199086304, valid_score=  0.1847\n",
      "Iteration 286, loss = 48142.12248721181, valid_score=  0.1915\n",
      "Iteration 287, loss = 48107.90762518612, valid_score=  0.1855\n",
      "Iteration 288, loss = 47977.09491825916, valid_score=  0.1916\n",
      "Iteration 289, loss = 47960.00806876262, valid_score=  0.1856\n",
      "Iteration 290, loss = 47825.71626314455, valid_score=  0.1914\n",
      "Iteration 291, loss = 47798.21027409453, valid_score=  0.1862\n",
      "Iteration 292, loss = 47674.199419475575, valid_score=  0.1908\n",
      "Iteration 293, loss = 47662.13388507521, valid_score=  0.1852\n",
      "Iteration 294, loss = 47535.22891770218, valid_score=  0.1914\n",
      "Iteration 295, loss = 47513.82399794573, valid_score=  0.1855\n",
      "Iteration 296, loss = 47396.19439878993, valid_score=  0.1917\n",
      "Iteration 297, loss = 47388.715920436174, valid_score=  0.1847\n",
      "Iteration 298, loss = 47268.69976271718, valid_score=  0.1921\n",
      "Iteration 299, loss = 47252.81714721449, valid_score=  0.1859\n",
      "Iteration 300, loss = 47141.180464424746, valid_score=  0.1923\n",
      "Iteration 301, loss = 47137.954727537566, valid_score=  0.1853\n",
      "Iteration 302, loss = 47024.32271321006, valid_score=  0.1929\n",
      "Iteration 303, loss = 47013.415076709665, valid_score=  0.1866\n",
      "Iteration 304, loss = 46907.4046898281, valid_score=  0.1919\n",
      "Iteration 305, loss = 46908.194007702616, valid_score=  0.1855\n",
      "Iteration 306, loss = 46800.431479344465, valid_score=  0.1926\n",
      "Iteration 307, loss = 46793.98336479797, valid_score=  0.1862\n",
      "Iteration 308, loss = 46693.247153308665, valid_score=  0.1924\n",
      "Iteration 309, loss = 46697.902106778754, valid_score=  0.1854\n",
      "Iteration 310, loss = 46595.48639289832, valid_score=  0.1924\n",
      "Iteration 311, loss = 46593.0060927214, valid_score=  0.1857\n",
      "Iteration 312, loss = 46497.20699522109, valid_score=  0.1919\n",
      "Iteration 313, loss = 46505.65720914494, valid_score=  0.1845\n",
      "Iteration 314, loss = 46408.06340631304, valid_score=  0.1917\n",
      "Iteration 315, loss = 46409.06213946266, valid_score=  0.1853\n",
      "Iteration 316, loss = 46317.888692835084, valid_score=  0.192\n",
      "Iteration 317, loss = 46330.13785430693, valid_score=  0.1846\n",
      "Iteration 318, loss = 46236.84575076463, valid_score=  0.1914\n",
      "Iteration 319, loss = 46240.80460631252, valid_score=  0.1841\n",
      "Iteration 320, loss = 46153.98979484678, valid_score=  0.1909\n",
      "Iteration 321, loss = 46170.11962643846, valid_score=  0.1839\n",
      "Iteration 322, loss = 46080.61837504904, valid_score=  0.1911\n",
      "Iteration 323, loss = 46086.94631526101, valid_score=  0.1849\n",
      "Iteration 324, loss = 46004.29153866618, valid_score=  0.1908\n",
      "Iteration 325, loss = 46024.47254704595, valid_score=  0.1835\n",
      "Iteration 326, loss = 45938.26304196905, valid_score=  0.1915\n",
      "Iteration 327, loss = 45946.2547443485, valid_score=  0.1849\n",
      "Iteration 328, loss = 45867.65487273896, valid_score=  0.1897\n",
      "Iteration 329, loss = 45892.14263283998, valid_score=  0.1843\n",
      "Iteration 330, loss = 45808.74579928754, valid_score=  0.1902\n",
      "Iteration 331, loss = 45817.572757685855, valid_score=  0.1854\n",
      "Iteration 332, loss = 45743.02789690025, valid_score=  0.1894\n",
      "Iteration 333, loss = 45772.08727403825, valid_score=  0.1852\n",
      "Iteration 334, loss = 45691.07680743947, valid_score=  0.1896\n",
      "Iteration 335, loss = 45699.90674220375, valid_score=  0.1857\n",
      "Iteration 336, loss = 45629.47176356874, valid_score=  0.1903\n",
      "Iteration 337, loss = 45663.150722255996, valid_score=  0.1853\n",
      "Iteration 338, loss = 45584.224253050736, valid_score=  0.191\n",
      "Iteration 339, loss = 45592.599945898844, valid_score=  0.186\n",
      "Iteration 340, loss = 45526.185439412606, valid_score=  0.1902\n",
      "Iteration 341, loss = 45564.04649049968, valid_score=  0.1851\n",
      "Iteration 342, loss = 45487.08102929796, valid_score=  0.191\n",
      "Iteration 343, loss = 45495.33803275499, valid_score=  0.1865\n",
      "Iteration 344, loss = 45432.45450756173, valid_score=  0.1905\n",
      "Iteration 345, loss = 45473.77377417934, valid_score=  0.1853\n",
      "Iteration 346, loss = 45398.7321281504, valid_score=  0.1913\n",
      "Iteration 347, loss = 45407.550929940655, valid_score=  0.1867\n",
      "Iteration 348, loss = 45347.5416847054, valid_score=  0.1908\n",
      "Iteration 349, loss = 45391.79192762092, valid_score=  0.1848\n",
      "Iteration 350, loss = 45318.58643916691, valid_score=  0.1912\n",
      "Iteration 351, loss = 45328.23298650501, valid_score=  0.1868\n",
      "Iteration 352, loss = 45270.708322760394, valid_score=  0.1914\n",
      "Iteration 353, loss = 45317.496919637924, valid_score=  0.1849\n",
      "Iteration 354, loss = 45246.01857378165, valid_score=  0.191\n",
      "Iteration 355, loss = 45256.669954109675, valid_score=  0.1874\n",
      "Iteration 356, loss = 45201.290954130796, valid_score=  0.1911\n",
      "Iteration 357, loss = 45250.37657189962, valid_score=  0.185\n",
      "Iteration 358, loss = 45180.48810119851, valid_score=  0.1917\n",
      "Iteration 359, loss = 45192.098305662395, valid_score=  0.1871\n",
      "Iteration 360, loss = 45138.67545339432, valid_score=  0.1906\n",
      "Iteration 361, loss = 45189.77227395666, valid_score=  0.1855\n",
      "Iteration 362, loss = 45121.35994108691, valid_score=  0.192\n",
      "Iteration 363, loss = 45134.04933057593, valid_score=  0.1874\n",
      "Iteration 364, loss = 45082.29802719126, valid_score=  0.1913\n",
      "Iteration 365, loss = 45135.255396493274, valid_score=  0.1859\n",
      "Iteration 366, loss = 45068.18616905904, valid_score=  0.1926\n",
      "Iteration 367, loss = 45081.7750046539, valid_score=  0.187\n",
      "Iteration 368, loss = 45031.625528548095, valid_score=  0.1923\n",
      "Iteration 369, loss = 45086.10829155178, valid_score=  0.1857\n",
      "Iteration 370, loss = 45020.32247569486, valid_score=  0.1929\n",
      "Iteration 371, loss = 45035.084907927034, valid_score=  0.1871\n",
      "Iteration 372, loss = 44986.19779388074, valid_score=  0.193\n",
      "Iteration 373, loss = 45042.32072686814, valid_score=  0.1863\n",
      "Iteration 374, loss = 44977.67916782015, valid_score=  0.1929\n",
      "Iteration 375, loss = 44993.0181690407, valid_score=  0.1863\n",
      "Iteration 376, loss = 44945.63284969661, valid_score=  0.1936\n",
      "Iteration 377, loss = 45003.07500544719, valid_score=  0.1858\n",
      "Iteration 378, loss = 44939.571265603256, valid_score=  0.1931\n",
      "Iteration 379, loss = 44956.16809340715, valid_score=  0.1859\n",
      "Iteration 380, loss = 44909.68884938149, valid_score=  0.194\n",
      "Iteration 381, loss = 44969.36905865572, valid_score=  0.184\n",
      "Iteration 382, loss = 44906.70381556338, valid_score=  0.1927\n",
      "Iteration 383, loss = 44922.55233374848, valid_score=  0.1849\n",
      "Iteration 384, loss = 44878.15347195478, valid_score=  0.1936\n",
      "Iteration 385, loss = 44938.17721579292, valid_score=  0.1825\n",
      "Iteration 386, loss = 44876.80035834147, valid_score=  0.1936\n",
      "Iteration 387, loss = 44895.366099580264, valid_score=  0.1846\n",
      "Iteration 388, loss = 44851.014997126265, valid_score=  0.1955\n",
      "Iteration 389, loss = 44912.95092597227, valid_score=  0.1819\n",
      "Iteration 390, loss = 44853.07925504906, valid_score=  0.1964\n",
      "Iteration 391, loss = 44866.283979944914, valid_score=  0.1854\n",
      "Iteration 392, loss = 44829.13194166581, valid_score=  0.1983\n",
      "Iteration 393, loss = 44874.511063772625, valid_score=  0.1836\n",
      "Iteration 394, loss = 44823.423739015685, valid_score=  0.1977\n",
      "Iteration 395, loss = 44846.347653753786, valid_score=  0.1856\n",
      "Iteration 396, loss = 44802.73286718475, valid_score=  0.1997\n",
      "Iteration 397, loss = 44848.53093250822, valid_score=  0.1856\n",
      "Iteration 398, loss = 44792.98891779572, valid_score=  0.1942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 399, loss = 44859.05588175332, valid_score=  0.1861\n",
      "Iteration 400, loss = 44787.494711524654, valid_score=  0.1936\n",
      "Iteration 401, loss = 44839.50099564972, valid_score=  0.1827\n",
      "Iteration 402, loss = 44775.6206307829, valid_score=  0.1967\n",
      "Iteration 403, loss = 44793.76955328476, valid_score=  0.1923\n",
      "Iteration 404, loss = 44751.939163227915, valid_score=  0.1978\n",
      "Iteration 405, loss = 44801.5323592962, valid_score=  0.1876\n",
      "Iteration 406, loss = 44746.39185450841, valid_score=  0.1949\n",
      "Iteration 407, loss = 44793.181571910594, valid_score=  0.1875\n",
      "Iteration 408, loss = 44736.53609689108, valid_score=  0.1935\n",
      "Iteration 409, loss = 44793.56218433655, valid_score=  0.1849\n",
      "Iteration 410, loss = 44734.60720980097, valid_score=  0.1942\n",
      "Iteration 411, loss = 44762.95842636683, valid_score=  0.1929\n",
      "Iteration 412, loss = 44718.677313664346, valid_score=  0.1948\n",
      "Iteration 413, loss = 44778.65596979632, valid_score=  0.188\n",
      "Iteration 414, loss = 44722.56337356126, valid_score=  0.194\n",
      "Iteration 415, loss = 44750.366963399574, valid_score=  0.1896\n",
      "Iteration 416, loss = 44706.24033688823, valid_score=  0.1954\n",
      "Iteration 417, loss = 44774.168564894506, valid_score=  0.1864\n",
      "Iteration 418, loss = 44716.79824494565, valid_score=  0.1943\n",
      "Iteration 419, loss = 44732.7163329312, valid_score=  0.1912\n",
      "Iteration 420, loss = 44697.06234111972, valid_score=  0.1946\n",
      "Iteration 421, loss = 44754.48065317821, valid_score=  0.1876\n",
      "Iteration 422, loss = 44700.91375719889, valid_score=  0.1948\n",
      "Iteration 423, loss = 44741.36566285068, valid_score=  0.1893\n",
      "Iteration 424, loss = 44692.760434052914, valid_score=  0.1949\n",
      "Iteration 425, loss = 44757.61536198362, valid_score=  0.1854\n",
      "Iteration 426, loss = 44702.017951476795, valid_score=  0.1949\n",
      "Iteration 427, loss = 44724.576228322636, valid_score=  0.189\n",
      "Iteration 428, loss = 44686.7161192622, valid_score=  0.1962\n",
      "Iteration 429, loss = 44750.87196458125, valid_score=  0.1863\n",
      "Iteration 430, loss = 44696.60888088667, valid_score=  0.1954\n",
      "Iteration 431, loss = 44724.89681809427, valid_score=  0.1887\n",
      "Iteration 432, loss = 44683.86840920769, valid_score=  0.1954\n",
      "Iteration 433, loss = 44754.82489759419, valid_score=  0.1865\n",
      "Iteration 434, loss = 44698.94462491493, valid_score=  0.1943\n",
      "Iteration 435, loss = 44716.125485771634, valid_score=  0.1914\n",
      "Iteration 436, loss = 44682.367243438464, valid_score=  0.1935\n",
      "Iteration 437, loss = 44738.6661626328, valid_score=  0.1889\n",
      "Iteration 438, loss = 44686.662253710034, valid_score=  0.1954\n",
      "Iteration 439, loss = 44732.00516337964, valid_score=  0.1905\n",
      "Iteration 440, loss = 44683.24775462502, valid_score=  0.1962\n",
      "Iteration 441, loss = 44738.7813683743, valid_score=  0.1884\n",
      "Iteration 442, loss = 44688.19372156523, valid_score=  0.1977\n",
      "Iteration 443, loss = 44730.71486493865, valid_score=  0.1889\n",
      "Iteration 444, loss = 44685.0336935253, valid_score=  0.2006\n",
      "Iteration 445, loss = 44754.751036706344, valid_score=  0.1869\n",
      "Iteration 446, loss = 44702.47565786789, valid_score=  0.1988\n",
      "Iteration 447, loss = 44716.43330543029, valid_score=  0.1909\n",
      "Iteration 448, loss = 44684.05168309792, valid_score=  0.1985\n",
      "Iteration 449, loss = 44734.21399902847, valid_score=  0.189\n",
      "Iteration 450, loss = 44685.38934339753, valid_score=  0.2001\n",
      "Iteration 451, loss = 44723.89434079892, valid_score=  0.1881\n",
      "Iteration 452, loss = 44679.58194675915, valid_score=  0.1992\n",
      "Iteration 453, loss = 44748.670713570194, valid_score=  0.1851\n",
      "Iteration 454, loss = 44693.57578358958, valid_score=  0.1946\n",
      "Iteration 455, loss = 44730.78564091772, valid_score=  0.187\n",
      "Iteration 456, loss = 44680.32268940132, valid_score=  0.1993\n",
      "Iteration 457, loss = 44754.34682529576, valid_score=  0.1892\n",
      "Iteration 458, loss = 44698.77971611585, valid_score=  0.1973\n",
      "Iteration 459, loss = 44702.84754081302, valid_score=  0.1955\n",
      "Iteration 460, loss = 44680.71092032006, valid_score=  0.1968\n",
      "Iteration 461, loss = 44715.8364596953, valid_score=  0.1929\n",
      "Iteration 462, loss = 44674.33523574918, valid_score=  0.1964\n",
      "Iteration 463, loss = 44765.13641266163, valid_score=  0.185\n",
      "Iteration 464, loss = 44704.209449615475, valid_score=  0.1962\n",
      "Iteration 465, loss = 44698.31725489644, valid_score=  0.1962\n",
      "Iteration 466, loss = 44686.269264443734, valid_score=  0.1982\n",
      "Iteration 467, loss = 44696.15058011774, valid_score=  0.1967\n",
      "Iteration 468, loss = 44675.02572703505, valid_score=  0.198\n",
      "Iteration 469, loss = 44722.93566231327, valid_score=  0.1905\n",
      "Iteration 470, loss = 44677.598643015226, valid_score=  0.1972\n",
      "Iteration 471, loss = 44783.44765991493, valid_score=  0.1833\n",
      "Iteration 472, loss = 44721.58821703744, valid_score=  0.1962\n",
      "Iteration 473, loss = 44703.49663331171, valid_score=  0.1968\n",
      "Iteration 474, loss = 44709.63694275948, valid_score=  0.1935\n",
      "Iteration 475, loss = 44689.92166711572, valid_score=  0.2004\n",
      "Iteration 476, loss = 44723.28266623766, valid_score=  0.1909\n",
      "Iteration 477, loss = 44686.5205457323, valid_score=  0.199\n",
      "Iteration 478, loss = 44770.15683467676, valid_score=  0.1852\n",
      "Iteration 479, loss = 44714.19999818709, valid_score=  0.1999\n",
      "Iteration 480, loss = 44709.9685898877, valid_score=  0.1948\n",
      "Iteration 481, loss = 44700.25026326711, valid_score=  0.1996\n",
      "Iteration 482, loss = 44707.15909271772, valid_score=  0.1963\n",
      "Iteration 483, loss = 44693.57947439185, valid_score=  0.2003\n",
      "Iteration 484, loss = 44715.165809946375, valid_score=  0.1929\n",
      "Iteration 485, loss = 44687.669563227784, valid_score=  0.2002\n",
      "Iteration 486, loss = 44763.88238126749, valid_score=  0.1871\n",
      "Iteration 487, loss = 44708.83843208854, valid_score=  0.1989\n",
      "Iteration 488, loss = 44760.34863219517, valid_score=  0.1855\n",
      "Iteration 489, loss = 44703.22179978052, valid_score=  0.2007\n",
      "Iteration 490, loss = 44830.32191669912, valid_score=  0.1806\n",
      "Iteration 491, loss = 44764.39498706945, valid_score=  0.1938\n",
      "Iteration 492, loss = 44724.600089403895, valid_score=  0.2017\n",
      "Iteration 493, loss = 44737.8735935578, valid_score=  0.1989\n",
      "Iteration 494, loss = 44708.044337158775, valid_score=  0.198\n",
      "Iteration 495, loss = 44758.45227851892, valid_score=  0.1893\n",
      "Iteration 496, loss = 44709.96443187384, valid_score=  0.1973\n",
      "Iteration 497, loss = 44789.33829188862, valid_score=  0.1827\n",
      "Iteration 498, loss = 44725.70061854764, valid_score=  0.2013\n",
      "Iteration 499, loss = 44740.73929624177, valid_score=  0.1948\n",
      "Iteration 500, loss = 44710.12063366461, valid_score=  0.2002\n",
      "Iteration 501, loss = 44765.313316406864, valid_score=  0.1922\n",
      "Iteration 502, loss = 44716.32259745925, valid_score=  0.2006\n",
      "Iteration 503, loss = 44776.67362569177, valid_score=  0.1897\n",
      "Iteration 504, loss = 44723.43783497329, valid_score=  0.1988\n",
      "Iteration 505, loss = 44779.386414082815, valid_score=  0.1888\n",
      "Iteration 506, loss = 44726.87444807058, valid_score=  0.1985\n",
      "Iteration 507, loss = 44780.18657509691, valid_score=  0.1902\n",
      "Iteration 508, loss = 44731.24528057542, valid_score=  0.1989\n",
      "Iteration 509, loss = 44781.31536500511, valid_score=  0.1915\n",
      "Iteration 510, loss = 44733.796154481555, valid_score=  0.1985\n",
      "Iteration 511, loss = 44791.4393705415, valid_score=  0.1893\n",
      "Iteration 512, loss = 44741.09512323692, valid_score=  0.2\n",
      "Iteration 513, loss = 44788.23494579755, valid_score=  0.1917\n",
      "Iteration 514, loss = 44742.06977663279, valid_score=  0.1992\n",
      "Iteration 515, loss = 44803.72469887218, valid_score=  0.1908\n",
      "Iteration 516, loss = 44752.89393030421, valid_score=  0.2004\n",
      "Iteration 517, loss = 44793.23511578885, valid_score=  0.1942\n",
      "Iteration 518, loss = 44750.57284072024, valid_score=  0.2002\n",
      "Iteration 519, loss = 44820.80501805263, valid_score=  0.1907\n",
      "Iteration 520, loss = 44767.852909847235, valid_score=  0.2018\n",
      "Iteration 521, loss = 44795.134696357025, valid_score=  0.1944\n",
      "Iteration 522, loss = 44759.53463174245, valid_score=  0.2013\n",
      "Iteration 523, loss = 44830.23648231546, valid_score=  0.1918\n",
      "Iteration 524, loss = 44777.47823146995, valid_score=  0.2014\n",
      "Iteration 525, loss = 44806.80072166573, valid_score=  0.193\n",
      "Iteration 526, loss = 44770.185943098164, valid_score=  0.1994\n",
      "Iteration 527, loss = 44844.002718577365, valid_score=  0.1913\n",
      "Iteration 528, loss = 44790.43989744269, valid_score=  0.1981\n",
      "Iteration 529, loss = 44815.728655610306, valid_score=  0.1929\n",
      "Iteration 530, loss = 44780.29880023235, valid_score=  0.2001\n",
      "Iteration 531, loss = 44842.753626110076, valid_score=  0.1931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 532, loss = 44791.74254326699, valid_score=  0.2016\n",
      "Iteration 533, loss = 44830.01788682776, valid_score=  0.1939\n",
      "Iteration 534, loss = 44789.3077104863, valid_score=  0.1993\n",
      "Iteration 535, loss = 44861.019703746584, valid_score=  0.1913\n",
      "Iteration 536, loss = 44808.18028057802, valid_score=  0.2021\n",
      "Iteration 537, loss = 44835.54671222891, valid_score=  0.1943\n",
      "Iteration 538, loss = 44800.05353383182, valid_score=  0.202\n",
      "Iteration 539, loss = 44878.63356587528, valid_score=  0.1917\n",
      "Iteration 540, loss = 44823.65182584604, valid_score=  0.2036\n",
      "Iteration 541, loss = 44843.53733188167, valid_score=  0.1941\n",
      "Iteration 542, loss = 44812.39194280773, valid_score=  0.2024\n",
      "Iteration 543, loss = 44884.49385169869, valid_score=  0.1887\n",
      "Iteration 544, loss = 44835.10702959579, valid_score=  0.1971\n",
      "Iteration 545, loss = 44850.77952747655, valid_score=  0.1958\n",
      "Iteration 546, loss = 44815.382132753555, valid_score=  0.2007\n",
      "Iteration 547, loss = 44876.09130459317, valid_score=  0.1942\n",
      "Iteration 548, loss = 44824.86814643228, valid_score=  0.2024\n",
      "Iteration 549, loss = 44872.93757743656, valid_score=  0.1926\n",
      "Iteration 550, loss = 44826.70956697607, valid_score=  0.1994\n",
      "Iteration 551, loss = 44907.77615174359, valid_score=  0.1869\n",
      "Iteration 552, loss = 44845.67018863119, valid_score=  0.2011\n",
      "Iteration 553, loss = 44862.17222316683, valid_score=  0.1966\n",
      "Iteration 554, loss = 44829.87437068608, valid_score=  0.2022\n",
      "Iteration 555, loss = 44887.05471899533, valid_score=  0.1947\n",
      "Iteration 556, loss = 44837.89482609951, valid_score=  0.1999\n",
      "Iteration 557, loss = 44888.16144210662, valid_score=  0.1922\n",
      "Iteration 558, loss = 44841.47138941345, valid_score=  0.1996\n",
      "Iteration 559, loss = 44908.09580472018, valid_score=  0.1911\n",
      "Iteration 560, loss = 44853.65423745218, valid_score=  0.202\n",
      "Iteration 561, loss = 44891.28086519001, valid_score=  0.1928\n",
      "Iteration 562, loss = 44849.219708902245, valid_score=  0.2008\n",
      "Iteration 563, loss = 44919.94325442937, valid_score=  0.1922\n",
      "Iteration 564, loss = 44866.5180920247, valid_score=  0.2012\n",
      "Iteration 565, loss = 44894.101458979385, valid_score=  0.1952\n",
      "Iteration 566, loss = 44858.49215632598, valid_score=  0.2021\n",
      "Iteration 567, loss = 44933.227127360835, valid_score=  0.1934\n",
      "Iteration 568, loss = 44878.3730061112, valid_score=  0.2014\n",
      "Iteration 569, loss = 44904.46978965289, valid_score=  0.1936\n",
      "Iteration 570, loss = 44869.7412131407, valid_score=  0.2011\n",
      "Iteration 571, loss = 44940.325746623486, valid_score=  0.1927\n",
      "Iteration 572, loss = 44886.719206930895, valid_score=  0.1998\n",
      "Iteration 573, loss = 44919.438747730754, valid_score=  0.1926\n",
      "Iteration 574, loss = 44880.92325552933, valid_score=  0.2014\n",
      "Iteration 575, loss = 44950.77809231844, valid_score=  0.1927\n",
      "Iteration 576, loss = 44896.79565522251, valid_score=  0.2009\n",
      "Iteration 577, loss = 44924.62000435686, valid_score=  0.1955\n",
      "Iteration 578, loss = 44889.848351157234, valid_score=  0.2012\n",
      "Iteration 579, loss = 44958.33887810202, valid_score=  0.1921\n",
      "Iteration 580, loss = 44905.63018004729, valid_score=  0.2013\n",
      "Iteration 581, loss = 44940.441481461094, valid_score=  0.1927\n",
      "Iteration 582, loss = 44901.62500600513, valid_score=  0.2008\n",
      "Iteration 583, loss = 44982.51292235335, valid_score=  0.1921\n",
      "Iteration 584, loss = 44926.30095187362, valid_score=  0.2024\n",
      "Iteration 585, loss = 44944.721727681565, valid_score=  0.1944\n",
      "Iteration 586, loss = 44914.1460913609, valid_score=  0.2022\n",
      "Iteration 587, loss = 44985.68493450661, valid_score=  0.1895\n",
      "Iteration 588, loss = 44934.692985796326, valid_score=  0.1985\n",
      "Iteration 589, loss = 44960.086702024404, valid_score=  0.1957\n",
      "Iteration 590, loss = 44917.55460813619, valid_score=  0.2019\n",
      "Iteration 591, loss = 44983.99618396192, valid_score=  0.1933\n",
      "Iteration 592, loss = 44929.91466346032, valid_score=  0.2047\n",
      "Iteration 593, loss = 44962.3510713994, valid_score=  0.1941\n",
      "Iteration 594, loss = 44923.20591130402, valid_score=  0.2022\n",
      "Iteration 595, loss = 45003.80133087824, valid_score=  0.1899\n",
      "Iteration 596, loss = 44948.18140289228, valid_score=  0.1995\n",
      "Iteration 597, loss = 44973.93670117258, valid_score=  0.1925\n",
      "Iteration 598, loss = 44932.99420060087, valid_score=  0.1989\n",
      "Iteration 599, loss = 44996.15320031878, valid_score=  0.1947\n",
      "Iteration 600, loss = 44942.71861032586, valid_score=  0.2037\n",
      "Iteration 601, loss = 44972.322774570035, valid_score=  0.1969\n",
      "Iteration 602, loss = 44935.264179716636, valid_score=  0.2032\n",
      "Iteration 603, loss = 45013.532732580825, valid_score=  0.1914\n",
      "Iteration 604, loss = 44958.73056362892, valid_score=  0.2\n",
      "Iteration 605, loss = 44984.044359529355, valid_score=  0.1925\n",
      "Iteration 606, loss = 44944.983854816615, valid_score=  0.2009\n",
      "Iteration 607, loss = 45014.70697283717, valid_score=  0.1923\n",
      "Iteration 608, loss = 44960.782465798904, valid_score=  0.2045\n",
      "Iteration 609, loss = 44987.300335851636, valid_score=  0.1957\n",
      "Iteration 610, loss = 44951.89249587098, valid_score=  0.2033\n",
      "Iteration 611, loss = 45033.309229505765, valid_score=  0.1903\n",
      "Iteration 612, loss = 44975.7659534403, valid_score=  0.1996\n",
      "Iteration 613, loss = 44996.99068024259, valid_score=  0.1938\n",
      "Iteration 614, loss = 44963.02877891086, valid_score=  0.2003\n",
      "Iteration 615, loss = 45025.83023372691, valid_score=  0.192\n",
      "Iteration 616, loss = 44973.69098703067, valid_score=  0.2015\n",
      "Iteration 617, loss = 45018.81247135642, valid_score=  0.1926\n",
      "Iteration 618, loss = 44973.28691862588, valid_score=  0.2015\n",
      "Iteration 619, loss = 45043.75738130384, valid_score=  0.1922\n",
      "Iteration 620, loss = 44988.3800424192, valid_score=  0.2015\n",
      "Iteration 621, loss = 45017.020426992385, valid_score=  0.1942\n",
      "Iteration 622, loss = 44980.95035137311, valid_score=  0.2034\n",
      "Iteration 623, loss = 45051.59060313753, valid_score=  0.193\n",
      "Iteration 624, loss = 44997.21060004566, valid_score=  0.2038\n",
      "Iteration 625, loss = 45030.85997431708, valid_score=  0.1955\n",
      "Iteration 626, loss = 44991.591329459094, valid_score=  0.2021\n",
      "Iteration 627, loss = 45074.087298641, valid_score=  0.1907\n",
      "Iteration 628, loss = 45015.603839428615, valid_score=  0.202\n",
      "Iteration 629, loss = 45031.183298407464, valid_score=  0.1955\n",
      "Iteration 630, loss = 45003.03012653573, valid_score=  0.2046\n",
      "Iteration 631, loss = 45064.395031958586, valid_score=  0.1919\n",
      "Iteration 632, loss = 45014.59097634873, valid_score=  0.198\n",
      "Iteration 633, loss = 45062.69918687246, valid_score=  0.1948\n",
      "Iteration 634, loss = 45011.72594753117, valid_score=  0.1992\n",
      "Iteration 635, loss = 45059.00575916404, valid_score=  0.1943\n",
      "Iteration 636, loss = 45010.97076233092, valid_score=  0.2043\n",
      "Iteration 637, loss = 45071.18371119119, valid_score=  0.1919\n",
      "Iteration 638, loss = 45019.77913068543, valid_score=  0.2027\n",
      "Iteration 639, loss = 45078.577824224456, valid_score=  0.191\n",
      "Iteration 640, loss = 45024.53303691209, valid_score=  0.2003\n",
      "Iteration 641, loss = 45089.05805031628, valid_score=  0.1921\n",
      "Iteration 642, loss = 45031.07784660198, valid_score=  0.2031\n",
      "Iteration 643, loss = 45061.392191407365, valid_score=  0.1959\n",
      "Iteration 644, loss = 45022.63919447784, valid_score=  0.2029\n",
      "Iteration 645, loss = 45094.02847755573, valid_score=  0.1938\n",
      "Iteration 646, loss = 45039.08927231784, valid_score=  0.2005\n",
      "Iteration 647, loss = 45070.97702985736, valid_score=  0.1933\n",
      "Iteration 648, loss = 45032.526102400814, valid_score=  0.2017\n",
      "Iteration 649, loss = 45107.533207575674, valid_score=  0.1935\n",
      "Iteration 650, loss = 45050.02608207472, valid_score=  0.2027\n",
      "Iteration 651, loss = 45072.75231847926, valid_score=  0.1966\n",
      "Iteration 652, loss = 45039.725485219045, valid_score=  0.2019\n",
      "Iteration 653, loss = 45109.531330675316, valid_score=  0.1925\n",
      "Iteration 654, loss = 45053.80753666238, valid_score=  0.2018\n",
      "Iteration 655, loss = 45094.53526696724, valid_score=  0.1935\n",
      "Iteration 656, loss = 45050.38670732185, valid_score=  0.2001\n",
      "Iteration 657, loss = 45125.446319790615, valid_score=  0.1933\n",
      "Iteration 658, loss = 45068.15975815207, valid_score=  0.2012\n",
      "Iteration 659, loss = 45091.557344501336, valid_score=  0.196\n",
      "Iteration 660, loss = 45059.59892113168, valid_score=  0.2024\n",
      "Iteration 661, loss = 45122.23058270697, valid_score=  0.1919\n",
      "Iteration 662, loss = 45069.80989484511, valid_score=  0.202\n",
      "Iteration 663, loss = 45118.362840587426, valid_score=  0.1947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 664, loss = 45071.53055801835, valid_score=  0.2021\n",
      "Iteration 665, loss = 45134.21766440306, valid_score=  0.1955\n",
      "Iteration 666, loss = 45081.57004129204, valid_score=  0.2024\n",
      "Iteration 667, loss = 45123.6678302908, valid_score=  0.1964\n",
      "Iteration 668, loss = 45080.579931882625, valid_score=  0.2031\n",
      "Iteration 669, loss = 45152.107789004775, valid_score=  0.1947\n",
      "Iteration 670, loss = 45096.679359358815, valid_score=  0.2029\n",
      "Iteration 671, loss = 45130.71840416577, valid_score=  0.1953\n",
      "Iteration 672, loss = 45091.22114286039, valid_score=  0.2032\n",
      "Iteration 673, loss = 45175.97473229763, valid_score=  0.1913\n",
      "Iteration 674, loss = 45115.37658444532, valid_score=  0.2016\n",
      "Iteration 675, loss = 45129.48067961891, valid_score=  0.1971\n",
      "Iteration 676, loss = 45103.77370664811, valid_score=  0.1974\n",
      "Iteration 677, loss = 45160.70962638192, valid_score=  0.1944\n",
      "Iteration 678, loss = 45104.902381264146, valid_score=  0.1994\n",
      "Iteration 679, loss = 45142.15428541794, valid_score=  0.1957\n",
      "Iteration 680, loss = 45098.14913399764, valid_score=  0.2043\n",
      "Iteration 681, loss = 45167.773893968544, valid_score=  0.1939\n",
      "Iteration 682, loss = 45112.41357372496, valid_score=  0.2033\n",
      "Iteration 683, loss = 45152.80088747801, valid_score=  0.1927\n",
      "Iteration 684, loss = 45109.31803789596, valid_score=  0.2025\n",
      "Iteration 685, loss = 45194.04051323178, valid_score=  0.1894\n",
      "Iteration 686, loss = 45129.95757644301, valid_score=  0.2024\n",
      "Iteration 687, loss = 45135.587405434664, valid_score=  0.1992\n",
      "Iteration 688, loss = 45114.38002618252, valid_score=  0.2052\n",
      "Iteration 689, loss = 45153.20013586012, valid_score=  0.1959\n",
      "Iteration 690, loss = 45110.45824491039, valid_score=  0.2033\n",
      "Iteration 691, loss = 45203.74108996352, valid_score=  0.1906\n",
      "Iteration 692, loss = 45142.274502365115, valid_score=  0.2012\n",
      "Iteration 693, loss = 45148.03046994902, valid_score=  0.1941\n",
      "Iteration 694, loss = 45129.53152420348, valid_score=  0.2025\n",
      "Iteration 695, loss = 45152.09920807345, valid_score=  0.1974\n",
      "Iteration 696, loss = 45121.96252329526, valid_score=  0.2031\n",
      "Iteration 697, loss = 45196.559407778084, valid_score=  0.1935\n",
      "Iteration 698, loss = 45139.20468436998, valid_score=  0.1998\n",
      "Iteration 699, loss = 45173.17892150204, valid_score=  0.1948\n",
      "Iteration 700, loss = 45130.237993988085, valid_score=  0.2027\n",
      "Iteration 701, loss = 45209.02931306533, valid_score=  0.1945\n",
      "Iteration 702, loss = 45149.145918121045, valid_score=  0.2028\n",
      "Iteration 703, loss = 45170.85743025811, valid_score=  0.1984\n",
      "Iteration 704, loss = 45139.01683600444, valid_score=  0.2035\n",
      "Iteration 705, loss = 45216.50033772367, valid_score=  0.1918\n",
      "Iteration 706, loss = 45156.06989981875, valid_score=  0.2008\n",
      "Iteration 707, loss = 45194.17484016933, valid_score=  0.1943\n",
      "Iteration 708, loss = 45149.76190693701, valid_score=  0.2019\n",
      "Iteration 709, loss = 45229.448733076635, valid_score=  0.1943\n",
      "Iteration 710, loss = 45169.33796877882, valid_score=  0.2025\n",
      "Iteration 711, loss = 45187.12947975755, valid_score=  0.1976\n",
      "Iteration 712, loss = 45159.34514357543, valid_score=  0.2033\n",
      "Iteration 713, loss = 45214.82274821815, valid_score=  0.1933\n",
      "Iteration 714, loss = 45164.616473287235, valid_score=  0.2014\n",
      "Iteration 715, loss = 45228.365711435945, valid_score=  0.1945\n",
      "Iteration 716, loss = 45173.29927508527, valid_score=  0.2029\n",
      "Iteration 717, loss = 45217.592321899676, valid_score=  0.1957\n",
      "Iteration 718, loss = 45171.55019297318, valid_score=  0.2035\n",
      "Iteration 719, loss = 45248.82323359213, valid_score=  0.1948\n",
      "Iteration 720, loss = 45189.47995944704, valid_score=  0.2021\n"
     ]
    }
   ],
   "source": [
    "nn = NNClassifier(n_classes=10, \n",
    "                  n_features=4096,\n",
    "                  n_hidden_units=20,\n",
    "                  epochs=1000,\n",
    "                  lr=0.0001, l2=1e2)\n",
    "nn = nn.fit(X_c, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2172"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.loadtxt(\"test_x.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.loadtxt(\"../input/test_x.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_c = np.array([[255 if x > 250 else 0 for x in y] for y in test_x])/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nn.predict(test_x_c)\n",
    "import pandas as pd\n",
    "dt = pd.DataFrame(data=predictions)\n",
    "dt.columns = [\"Label\"]\n",
    "dt = dt.astype(int)\n",
    "dt.to_csv('../output/nn_predictions.csv', mode='w',index=True, index_label='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 5,\n",
       " 3,\n",
       " 8,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 0,\n",
       " 8,\n",
       " 9,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 7,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 8,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 7,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 4,\n",
       " 8,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 7,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 6,\n",
       " 3,\n",
       " 7,\n",
       " 1,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 0,\n",
       " 7,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 7,\n",
       " 5,\n",
       " 7,\n",
       " 8,\n",
       " 3,\n",
       " 2,\n",
       " 6,\n",
       " 7,\n",
       " 0,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 4,\n",
       " 7,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 7,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 7,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 6,\n",
       " 9,\n",
       " 8,\n",
       " 4,\n",
       " 9,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 9,\n",
       " 7,\n",
       " 3,\n",
       " 7,\n",
       " 5,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 7,\n",
       " 3,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 6,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 0,\n",
       " 6,\n",
       " 8,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 9,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 9,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 3,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 7,\n",
       " 1,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 9,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 7,\n",
       " 0,\n",
       " 3,\n",
       " 7,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 3,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 7,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 0,\n",
       " 6,\n",
       " 7,\n",
       " 3,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 7,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 9,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 3,\n",
       " 7,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 9,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 2,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 4,\n",
       " 9,\n",
       " 4,\n",
       " 3,\n",
       " 7,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 8,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 7,\n",
       " 0,\n",
       " 6,\n",
       " 9,\n",
       " 7,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 0,\n",
       " 6,\n",
       " 1,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 1,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 8,\n",
       " 3,\n",
       " 6,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 9,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 7,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 7,\n",
       " 6,\n",
       " 8,\n",
       " 4,\n",
       " 7,\n",
       " 5,\n",
       " 9,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 6,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 7,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 7,\n",
       " 7,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 6,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 9,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 8,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 2,\n",
       " 7,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 7,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 9,\n",
       " 4,\n",
       " 0,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 1,\n",
       " 8,\n",
       " 9,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 1,\n",
       " 9,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 9,\n",
       " 6,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 6,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 7,\n",
       " 4,\n",
       " 8,\n",
       " 1,\n",
       " 3,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 2,\n",
       " 7,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 6,\n",
       " 7,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 7,\n",
       " 3,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 6,\n",
       " 9,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 7,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 8,\n",
       " 2,\n",
       " 9,\n",
       " 6,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 1,\n",
       " 6,\n",
       " 8,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 7,\n",
       " 9,\n",
       " 0,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 7,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 0,\n",
       " 9,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 6,\n",
       " 9,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 2,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 7,\n",
       " 5,\n",
       " 3,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 7,\n",
       " 8,\n",
       " 4,\n",
       " 9,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 9,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 8,\n",
       " 9,\n",
       " 4,\n",
       " 9,\n",
       " 8,\n",
       " 1,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 3,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 2,\n",
       " 6,\n",
       " 1,\n",
       " 8,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 7,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 4,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 1,\n",
       " 7,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 6,\n",
       " 8,\n",
       " 9,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 8,\n",
       " 9,\n",
       " 1,\n",
       " 2,\n",
       " 8,\n",
       " 0,\n",
       " 1,\n",
       " 6,\n",
       " 7,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 9,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 6,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 8,\n",
       " 1,\n",
       " 9,\n",
       " 7,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 7,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
